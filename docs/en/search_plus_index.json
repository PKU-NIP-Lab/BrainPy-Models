{"./":{"url":"./","title":"0. Introduction","keywords":"","body":"BrainPy Introduction In this chapter, we will briefly introduce how to implement computational neuroscience models with BrainPy. For more detailed documents and tutorials, please check our Github repository BrainPy and BrainModels. BrainPy is a Python platform for computational neuroscience and brain-inspired computation. To model with BrainPy, users should follow 3 steps: 1) Define Python classes for neuron and synapse models. BrainPy provides base classes for different kinds of models, users only need to inherit from those base classes, and define specific methods to tell BrainPy what operations they want the models to take during the simulation. In this process, BrainPy will assist users in the numerical integration of differential equations (ODE, SDE, etc.), adaptation of various backends (Numpy, PyTorch, etc.), and other functions to simplify code logic. 2) Instantiate Python classes as objects of neuron group and synapse connection groups, pass the instantiated objects to BrainPy class Network, and call method run to simulate the network. 3) Call BrainPy modules like the measure module and the visualize module to display the simulation results. With this overall concept of BrainPy, we will go into more detail about implementations in the following sections. In neural systems, neurons are connected by synapses to build networks, so we will introduce neuron models, synapse models, and network models in order. "},"neurons.html":{"url":"neurons.html","title":"1. Neuron models","keywords":"","body":"1. Neuron models Neuron models can be classified into three types from complex to simple: biophysical models, reduced models and firing rate models. 1.1 Biological Background 1.2 Biophysical models 1.3 Reduced models 1.4 Firing rate models "},"neurons/biological_background.html":{"url":"neurons/biological_background.html","title":"1.1 Biological background","keywords":"","body":"1.1 Biological backgrounds As the basic unit of neural systems, neurons maintain mystique to researchers for a long while. In recent centuries, however, along with the development of experimental techniques, researchers have painted a general figure of those little things working ceaselessly in our neural system. To achieve our final goal of modeling neurons with computational neuroscience methods, we may start with a patch of real neuron membrane. Fig. 1-1 Neuron membrane diagram | what-when-how.com The figure above is a general diagram of neuron membrane with phospholipid bilayer and ion channels. The membrane divides the ions and fluid into intracellular and extracellular, partially prevent them from exchanging, thus generates membrane potential---- the difference in electric potential across the membrane. An ion in the fluid is subjected to two forces. The force of diffusion is caused by the ion concentration difference across the membrane, while the force of electric field is caused by the electric potential difference. When these two forces reach balance, the total forces on ions are 0, and each type of ion meets an equilibrium potential, while the neuron holds a membrane potential lower than 0. This membrane potential integrated by all those ion equilibrium potentials is the resting potential, and the neuron is, in a so-called resting state. If the neuron is not disturbed, it will just come to the balanced resting state, and rest. However, our neural system receives countless inputs every millisecond, from external inputs to recurrent inputs, from specific stimulus inputs to non-specific background inputs. Receiving all these inputs, neurons generate action potentials (or spikes) to transfer and process information all across the neural system. Fig. 1-2 Action Potential | Wikipedia Passing through the ion channels shown in Fig.1-1, ions on both sides of the hydrophobic phospholipid bilayer are exchanged. Due to changes in the environment caused by, for example, an external input, ion channels will switch between their open/close states, therefore allow/prohibit ion exchanges. During the switch, the ion concentrations (mainly Na+ and K+) change, induce a significant change on neuron's membrane potential: the membrane potential will raise to a peak value and then fall back in a short time period. Biologically, when such a series of potential changes happens, we say the neuron generates an action potential or a spike, or the neuron fires. An action potential can be mainly divided into three periods: depolarization, repolarization and refractory period. During the depolarization period, Na+ flow into the neuron and K+ flow out of the neuron, however the inflow of Na+ is faster, so the membrane potential raises from a low value VrestV_{rest}V​rest​​ to a value much higher called VthV_{th}V​th​​, then the outflow of K+ becomes faster than Na+, and the membrane potential is reset to a value lower than resting potential during the repolarization period. After that, because of the relatively lower membrane potential, the neuron is unlikely to generate another spike immediately, until the refractory period passes. A single action potential is complex enough, but in our neural system, one single neuron can generate several action potentials in less than a second. How, exactly, do the neurons fire? Different kinds of neurons may spike when facing different inputs, and the pattern of their spiking can be classified into several firing patterns, some of which are shown in the following figure. Figure 1-3 Some firing patterns Those firing patterns, together with the shape of action potentials, are what computational neuroscience wants to model at the cellular level. "},"neurons/biophysical_models.html":{"url":"neurons/biophysical_models.html","title":"1.2 Biophysical models","keywords":"","body":"1.2 Biophysical models 1.2.1 Hodgkin-Huxley model Hodgkin and Huxley (1952) recorded the generation of action potential on squid giant axons with voltage clamp technique, and proposed the canonical neuron model called Hodgin-Huxley model (HH model). In last section we have introduced a general template for neuron membrane. Computational neuroscientists always model neuron membrane as equivalent circuit like the following figure. Fig. 1-4 Equivalent circuit diagram | NeuroDynamics The equivalent circuit diagram of Fig.1-1 is shown in Fig. 1-4, in which the patch of neuron membrane is converted into electric components. In Fig.1-4, the capacitance CCC refers to the hydrophobic phospholipid bilayer with low conductance, and current III refers to the external stimulus. As Na+ ion channels and K+ ion channels are important in the generation of action potentials, these two ion channels are modeled as the two variable resistances RNaR_{Na}R​Na​​ and RKR_KR​K​​ in parallel on the right side of the circuit diagram, and the resistance RRR refers to all the non-specific ion channels on the membrane. The batteries ENaE_{Na}E​Na​​, EKE_KE​K​​ and ELE_LE​L​​ refer to the electric potential differences caused by the concentration differences of corresponding ions. Consider the Kirchhoff’s first law, that is, for any node in an electrical circuit, the sum of currents flowing into that node is equal to the sum of currents flowing out of that node, Fig. 1-4 can be modeled as differential equations: CdVdt=−(g¯Nam3h(V−ENa)+g¯Kn4(V−EK)+gleak(V−Eleak))+I(t) C \\frac{dV}{dt} = -(\\bar{g}_{Na} m^3 h (V - E_{Na}) + \\bar{g}_K n^4(V - E_K) + g_{leak}(V - E_{leak})) + I(t) C​dt​​dV​​=−(​g​¯​​​Na​​m​3​​h(V−E​Na​​)+​g​¯​​​K​​n​4​​(V−E​K​​)+g​leak​​(V−E​leak​​))+I(t) dxdt=αx(1−x)−βx,x∈{Na,K,leak} \\frac{dx}{dt} = \\alpha_x(1-x) - \\beta_x , x \\in \\{ Na, K, leak \\} ​dt​​dx​​=α​x​​(1−x)−β​x​​,x∈{Na,K,leak} That is the HH model. Note that in the first equation above, the first three terms on the right hand are the current go through Na+ ion channels, K+ ion channels and other non-specific ion channels, respectively, while I(t)I(t)I(t) is an external input. On the left hand, CdVdt=dQdt=IC\\frac{dV}{dt} = \\frac{dQ}{dt} = IC​dt​​dV​​=​dt​​dQ​​=I is the current go through the capacitance. In the computing of ion channel currents, other than the Ohm's law I=U/R=gUI = U/R = gUI=U/R=gU, HH model introduces three gating variables m, n and h to control the open/close state of ion channels. To be precise, variables m and h control the state of Na+ ion channel, variable n controls the state of K+ ion channel, and the real conductance of an ion channel is the product of maximal conductance g¯\\bar{g}​g​¯​​ and the state of gating variables. Gating variables' dynamics can be expressed in a Markov-like form, in which αx\\alpha_xα​x​​ refers to the activation rate of gating variable x, and βx\\beta_xβ​x​​ refers to the de-activation rate of x. The expressions of αx\\alpha_xα​x​​ and βx\\beta_xβ​x​​ (as shown in equations below) are fitted by experimental data. αm(V)=0.1(V+40)1−exp(−(V+40)10) \\alpha_m(V) = \\frac{0.1(V+40)}{1 - exp(\\frac{-(V+40)}{10})} α​m​​(V)=​1−exp(​10​​−(V+40)​​)​​0.1(V+40)​​ βm(V)=4.0exp(−(V+65)18) \\beta_m(V) = 4.0 exp(\\frac{-(V+65)}{18}) β​m​​(V)=4.0exp(​18​​−(V+65)​​) αh(V)=0.07exp(−(V+65)20) \\alpha_h(V) = 0.07 exp(\\frac{-(V+65)}{20}) α​h​​(V)=0.07exp(​20​​−(V+65)​​) βh(V)=11+exp(−(V+35)10) \\beta_h(V) = \\frac{1}{1 + exp(\\frac{-(V + 35)}{10})} β​h​​(V)=​1+exp(​10​​−(V+35)​​)​​1​​ αn(V)=0.01(V+55)1−exp(−(V+55)10) \\alpha_n(V) = \\frac{0.01(V+55)}{1 - exp(\\frac{-(V+55)}{10})} α​n​​(V)=​1−exp(​10​​−(V+55)​​)​​0.01(V+55)​​ βn(V)=0.125exp(−(V+65)80) \\beta_n(V) = 0.125 exp(\\frac{-(V+65)}{80}) β​n​​(V)=0.125exp(​80​​−(V+65)​​) Run codes in our github repository: https://github.com/PKU-NIP-Lab/BrainModels The V-t plot of HH model simulated by BrainPy is shown below. The three periods, depolarization, repolarization and refractory period of a real action potential can be seen in the V-t plot. In addition, during the depolarization period, the membrane integrates external inputs slowly at first, and increases rapidly once it grows beyond some point, which also reproduces the \"shape\" of action potentials. "},"neurons/reduced_models.html":{"url":"neurons/reduced_models.html","title":"1.3 Reduced models","keywords":"","body":"1.3 Reduced models Inspired by biophysical experiments, Hodgkin-Huxley model is precise but costly. Researchers proposed the reduced models to reduce the consumption on computing resources and running time in simulation. These models are simple and easy to compute, while they can still reproduce the main pattern of neuron behaviors. Although their representation capabilities are not as good as biophysical models, such a loss of accuracy is sometimes acceptable considering their simplicity. 1.3.1 Leaky Integrate-and-Fire model The most typical reduced model is the Leaky Integrate-and-Fire model (LIF model) presented by Lapicque (1907). LIF model is a combination of integrate process represented by differential equation and spike process represented by conditional judgment: τdVdt=−(V−Vrest)+RI(t) \\tau\\frac{dV}{dt} = - (V - V_{rest}) + R I(t) τ​dt​​dV​​=−(V−V​rest​​)+RI(t) If V>VthV > V_{th}V>V​th​​, neuron fires, V←Vreset V \\gets V_{reset} V←V​reset​​ τ=RC\\tau = RCτ=RC is the time constant of LIF model, the larger τ\\tauτ is, the slower model dynamics is. The equation shown above is corresponding to a simpler equivalent circuit than HH model, for it does not model the Na+ and K+ ion channels any more. Actually, in LIF model, only the consistence RRR, capacitance CCC, battery EEE and external input III is modeled. Fig1-4 Equivalent circuit of LIF model Compared with HH model, LIF model does not model the shape of action potentials, which means, the membrane potential does not burst before a spike. Also, the refractory period is overlooked in the original model, and in order to generate it, another conditional judgment must be added: If t−tlastspike=refractoryperiod t-t_{last spike}t−t​lastspike​​=refractoryperiod then neuron is in refractory period, membrane potential VVV will not be updated. 1.3.2 Quadratic Integrate-and-Fire model To pursue higher representation capability, Latham et al. (2000) proposed Quadratic Integrate-and-Fire model (QuaIF model), in which they add a second order term in differential equation so the neurons can generate spike better. τdVdt=a0(V−Vrest)(V−Vc)+RI(t) \\tau\\frac{d V}{d t}=a_0(V-V_{rest})(V-V_c) + RI(t) τ​dt​​dV​​=a​0​​(V−V​rest​​)(V−V​c​​)+RI(t) In the equation above, a0a_0a​0​​ is a parameter controls the slope of membrane potential before a spike, and VcV_cV​c​​ is the critical potential for action potential initialization. Below VCV_CV​C​​, membrane potential VVV increases slowly, once it grows beyond VCV_CV​C​​, VVV turns to rapid increase. 1.3.3 Exponential Integrate-and-Fire model Exponential Integrate-and-Fire model (ExpIF model) (Fourcaud-Trocme et al., 2003) is more expressive than QuaIF model. With the exponential term added to the right hand of differential equation, the dynamics of ExpIF model can now generates a more realistic action potential. τdVdt=−(V−Vrest)+ΔTeV−VTΔT+RI(t) \\tau \\frac{dV}{dt} = - (V - V_{rest}) + \\Delta_T e^{\\frac{V - V_T}{\\Delta_T}} + R I(t) τ​dt​​dV​​=−(V−V​rest​​)+Δ​T​​e​​Δ​T​​​​V−V​T​​​​​​+RI(t) In the exponential term, VTV_TV​T​​ is the critical potential of generating action potential, below which VVV increases slowly and above which rapidly. ΔT\\Delta_TΔ​T​​ is the slope of action potentials in ExpIF model, and when ΔT→0\\Delta_T\\to 0Δ​T​​→0, the shape of spikes in ExpIF model will be equivalent to the LIF model with Vth=VTV_{th} = V_TV​th​​=V​T​​(Fourcaud-Trocme et al., 2003) . 1.3.4 Adaptive Exponential Integrate-and-Fire model While facing a constant stimulus, the response generated by a single neuron will sometimes decreases over time, this phenomenon is called adaptation in biology. To reproduce the adaptation behavior of neurons, researchers add a weight variable www to existing integrate-and-fire models like LIF, QuaIF and ExpIF models. Here we introduce a typical one: Adaptive Exponential Integrate-and-Fire model (AdExIF model) (Gerstner et al., 2014). τmdVdt=−(V−Vrest)+ΔTeV−VTΔT−Rw+RI(t) \\tau_m \\frac{dV}{dt} = - (V - V_{rest}) + \\Delta_T e^{\\frac{V - V_T}{\\Delta_T}} - R w + R I(t) τ​m​​​dt​​dV​​=−(V−V​rest​​)+Δ​T​​e​​Δ​T​​​​V−V​T​​​​​​−Rw+RI(t) τwdwdt=a(V−Vrest)−w+bτw∑δ(t−tf)) \\tau_w \\frac{dw}{dt} = a(V - V_{rest})- w + b \\tau_w \\sum \\delta(t - t^f)) τ​w​​​dt​​dw​​=a(V−V​rest​​)−w+bτ​w​​∑δ(t−t​f​​)) The first differential equation of AdExIF model, as the model's name shows, is similar to ExpIF model we introduced above, except for the term of adaptation, which is shown as −Rw-Rw−Rw in the equation. The weight term www is regulated by the second differential equation. aaa describes the sensitivity of the recovery variable www to the sub-threshold fluctuations of VVV, and bbb is the increment value of www generated by a spike, and www will also decay over time. Give AdExIF neuron a constant input, after several spikes, the value of www will increase to a high value, which slows down the rising speed of VVV, thus reduces the neuron's firing rate. 1.3.5 Hindmarsh-Rose model To simulate the bursting spike pattern in neurons (i.e., continuously firing in a short time period), Hindmarsh and Rose (1984) proposed Hindmarsh-Rose model, import a third model variable zzz as slow variable to control the bursting of neuron. dVdt=y−aV3+bV2−z+I \\frac{d V}{d t} = y - a V^3 + b V^2 - z + I ​dt​​dV​​=y−aV​3​​+bV​2​​−z+I dydt=c−dV2−y \\frac{d y}{d t} = c - d V^2 - y ​dt​​dy​​=c−dV​2​​−y dzdt=r(s(V−Vrest)−z) \\frac{d z}{d t} = r (s (V - V_{rest}) - z) ​dt​​dz​​=r(s(V−V​rest​​)−z) The VVV variable refers to membrane potential, and yyy, zzz are two gating variables. The parameter bbb in dVdt\\frac{dV}{dt}​dt​​dV​​ equation allows the model to switch between spiking and bursting states, and controls the spiking frequency. rrr controls slow variable zzz's variation speed, affects the number of spikes per burst when bursting, and governs the spiking frequency together with bbb. The parameter sss governs adaptation, and other parameters are fitted by firing patterns. In the variable-t plot painted below, we may see that the slow variable zzz changes much slower than VVV and yyy. Also, VVV and yyy are changing periodically during the simulation. With the theoretical analysis module analysis of BrainPy, we may explain the existence of this periodicity through theoretical analysis. In Hindmarsh-Rose model, the trajectory of VVV and yyy approaches a limit cycle in phase plane, therefore their values change periodically along the limit cycle. 1.3.6 Generalized Integrate-and-Fire model Generalized Integrate-and-Fire model (GIF model) (Mihalaş et al., 2009) integrates several firing patterns in one model. With 4 model variables, it can generate more than 20 types of firing patterns, and is able to alternate between patterns by fitting parameters. dIjdt=−kjIj,j=1,2 \\frac{d I_j}{d t} = - k_j I_j, j = {1, 2} ​dt​​dI​j​​​​=−k​j​​I​j​​,j=1,2 τdVdt=(−(V−Vrest)+R∑jIj+RI) \\tau \\frac{d V}{d t} = ( - (V - V_{rest}) + R\\sum_{j}I_j + RI) τ​dt​​dV​​=(−(V−V​rest​​)+R​j​∑​​I​j​​+RI) dVthdt=a(V−Vrest)−b(Vth−Vth∞) \\frac{d V_{th}}{d t} = a(V - V_{rest}) - b(V_{th} - V_{th\\infty}) ​dt​​dV​th​​​​=a(V−V​rest​​)−b(V​th​​−V​th∞​​) When VVV meets VthV_{th}V​th​​, Generalized IF neuron fire: Ij←RjIj+Aj I_j \\leftarrow R_j I_j + A_j I​j​​←R​j​​I​j​​+A​j​​ V←Vreset V \\leftarrow V_{reset} V←V​reset​​ Vth←max(Vthreset,Vth) V_{th} \\leftarrow max(V_{th_{reset}}, V_{th}) V​th​​←max(V​th​reset​​​​,V​th​​) In the dVdt\\frac{dV}{dt} ​dt​​dV​​ differential equation, just like all the integrate-and-fire models, τ\\tauτ is time constant, VVV is membrane potential, VrestV_{rest}V​rest​​ is resting potential, RRR is conductance, and III is external input. However, in GIF model, variable amounts of internal currents are added to the equation, shown as the ∑jIj\\sum_j I_j∑​j​​I​j​​ term. Each Ij I_j I​j​​ is an internal current in the neuron, with a decay rate of kjk_jk​j​​. RjR_jR​j​​ and AjA_jA​j​​ are free parameters, RjR_jR​j​​ describes the dependence of IjI_jI​j​​ reset value on the value of IjI_jI​j​​ before spike, and AjA_jA​j​​ is a constant value added to the reset value after spike. The variable threshold potential VthV_{th}V​th​​ is regulated by two parameters: aaa describes the dependence of VthV_{th}V​th​​ on the membrane potential VVV, and bbb is the rate VthV_{th}V​th​​ approaches the infinite value of threshold Vth∞V_{th_{\\infty}}V​th​∞​​​​. VthresetV_{th_{reset}}V​th​reset​​​​ is the reset value of threshold potential when neuron fires. "},"neurons/firing_rate_models.html":{"url":"neurons/firing_rate_models.html","title":"1.4 Firing rate models","keywords":"","body":"1.4 Firing Rate models Firing Rate models are simpler than reduced models. In these models, each compute unit represents a neuron group, the membrane potential variable VVV in single neuron models is replaced by firing rate variable aaa (or rrr or ν\\nuν). Here we introduce a canonical firing rate unit. 1.4.1 Firing Rate Unit Wilson and Cowan (1972) proposed this unit to represent the activities in excitatory and inhibitory cortical neuron columns. Each element of variables aea_ea​e​​ and aia_ia​i​​ refers to the average activity of a neuron column group contains multiple neurons. τedae(t)dt=−ae(t)+(ke−re∗ae(t))∗S(c1ae(t)−c2ai(t)+Iexte(t)) \\tau_e \\frac{d a_e(t)}{d t} = - a_e(t) + (k_e - r_e * a_e(t)) * \\mathcal{S}(c_1 a_e(t) - c_2 a_i(t) + I_{ext_e}(t)) τ​e​​​dt​​da​e​​(t)​​=−a​e​​(t)+(k​e​​−r​e​​∗a​e​​(t))∗S(c​1​​a​e​​(t)−c​2​​a​i​​(t)+I​ext​e​​​​(t)) τidai(t)dt=−ai(t)+(ki−ri∗ai(t))∗S(c3ae(t)−c4ai(t)+Iexti(t)) \\tau_i \\frac{d a_i(t)}{d t} = - a_i(t) + (k_i - r_i * a_i(t)) * \\mathcal{S}(c_3 a_e(t) - c_4 a_i(t) + I_{ext_i}(t)) τ​i​​​dt​​da​i​​(t)​​=−a​i​​(t)+(k​i​​−r​i​​∗a​i​​(t))∗S(c​3​​a​e​​(t)−c​4​​a​i​​(t)+I​ext​i​​​​(t)) S(x)=11+exp(−a(x−θ))−11+exp(aθ) \\mathcal{S}(x) = \\frac{1}{1 + exp(- a(x - \\theta))} - \\frac{1}{1 + exp(a\\theta)} S(x)=​1+exp(−a(x−θ))​​1​​−​1+exp(aθ)​​1​​ The subscript x∈{e,i}x\\in\\{e, i\\}x∈{e,i} points out whether this parameter or variable corresponds to excitatory or inhibitory neuron group. In the differential equations, τx\\tau_xτ​x​​ refers to the time constant of neuron columns, parameters kxk_xk​x​​ and rxr_xr​x​​ control the refractory periods, axa_xa​x​​ and θx\\theta_xθ​x​​ refer to the slope factors and phase parameters of sigmoid functions, and external inputs IextxI_{ext_{x}}I​ext​x​​​​ are given separately to excitatory and inhibitory neuron groups. "},"synapses.html":{"url":"synapses.html","title":"2. Synapse models","keywords":"","body":"2. Synapse models When we model the firing of neurons, we need to connect them. Synapse is very important for the communication between neurons, and it is an essential component of the formation of the network. Therefore, we need to model the synapse. We will first introduce how to implement synaptic dynamics with BrainPy, then introduce synapse plasticity. 2.1 Synaptic Models 2.2 Plasticity Models "},"synapse/dynamics.html":{"url":"synapse/dynamics.html","title":"2.1 Synaptic models","keywords":"","body":"2.1 Synaptic Models In the previous section, we learned how to model neurons and their action potentials. In this section, we will focus on how neurons communicate. 2.1.1 Chemical Synapses Biological Background Fig. 2-1 shows the biological process of information transmission between neurons. The action potential of the presynaptic neuron makes the axon terminal release neurotransmitters (also called transmitters) into the synaptic cleft, and then the membrane potential of the postsynaptic cell changes after a brief delay. These changes are called postsynaptic potentials (PSP), and they can be either excitatory or inhibitory depending on the type of transmitter. Glutamate is one of the important excitatory neurotransmitters, and Gamma-aminobutyric acid (GABA) is one of the important inhibitory neurotransmitters. Neurotransmitters affect their targets by interacting with receptors on the postsynaptic membrane. When the transmitter binds to the receptor, it would either open an ion channel (ionotropic receptors) or alter chemical reactions within the target cell (metabotropic receptors). In this section, we will introduce how to model some common synapses and their implementations with BrainPy: AMPA and NMDA receptors are both ionotropic receptors of Glutamate, but the NMDA receptor are typically blocked by magnesium ions (Mg2+^{2+}​2+​​) and cannot respond to the glutamate. With repeated activation of AMPA receptors, the change in postsynaptic potential drives Mg2+^{2+}​2+​​ out of NMDA channel, then the NMDA receptors are able to respond to glutamate. Therefore, the dynamics of NMDA is much slower than that of AMPA. GABAA and GABAB are two classes of GABA receptors. GABAA receptors are ionotropic, typically producing fast inhibitory postsynaptic potential; while GABAB receptors are metabotropic receptors, typically producing a slow-occurring inhibitory postsynaptic potential. Fig. 2-1 Biological Synapse (Adaptive from Gerstner et al., 2014 1) In order to keep things simple, we use gating variable s to describe how many portion of ion channels will open whenever a presynaptic spike arrives while modeling. We will first introduce AMPA receptor as an example to show how to develop synapse models and implement them with BrainPy. AMPA Synapse As we mentioned before, the AMPA receptor is an ionotropic receptor, that is, when a neurotransmitter binds to it, the ion channel will be opened immediately to allow Na+^+​+​​ and K+^+​+​​ ions flux. We can use Markov process to describe the opening and closing process of ion channels. As shown in Fig. 2-2, sss represents the probability of channel opening, 1−s1-s1−s represents the probability of ion channel closing, and α\\alphaα and β\\betaβ are the transition probability. Because neurotransmitters can open ion channels, the transfer probability from 1−s1-s1−s to sss is affected by the concentration of neurotransmitters. We denote the concentration of neurotransmitters as [T]. Fig. 2-2 Markov process of channel dynamics We obtained the following formula when describing the process by a differential equation. dsdt=α[T](1−s)−βs \\frac {ds}{dt} = \\alpha [T] (1-s) - \\beta s ​dt​​ds​​=α[T](1−s)−βs Where α[T]\\alpha [T]α[T] denotes the transition probability from state (1−s)(1-s)(1−s) to state (s)(s)(s); and β\\betaβ represents the transition probability of the other direction. Now let's see how to implement such a model with BrainPy. First of all, we need to define a class that inherits from bp.TwoEndConn, because synapses connect two neurons. Within the class, we can define the differential equation with derivative function, this is the same as the definition of neuron models. Then we use the __ init__ Function to initialize the required parameters and variables. We update sss by an update function. After the implementation, we can plot the graph of sss changing with time. We would first write a run_syn function to run and plot the graph. To run a synapse, we need neuron groups, so we use the LIF neuron provided by brainmodels package. Then we would expect to see the following result: As can be seen from the above figure, when the presynaptic neurons fire, the value of sss will first increase, and then decay. Alpha、Exponential Synapses Because many synaptic models have the same dynamic characteristics as AMPA synapses, sometimes we don't need to use models that specifically correspond to biological synapses. Therefore, some abstract synaptic models have been proposed. Here, we will introduce the implementation of four abstract models on BrainPy. These models can also be found in the Brain-Models package. (1) Differences of two exponentials Let's first introduce the Differences of two exponentials model, its dynamics is given by, s=τ1τ2τ1−τ2(exp(−t−tsτ1)−exp(−t−tsτ2)) s = \\frac {\\tau_1 \\tau_2}{\\tau_1 - \\tau_2} (\\exp(-\\frac{t - t_s}{\\tau_1}) - \\exp(-\\frac{t - t_s}{\\tau_2})) s=​τ​1​​−τ​2​​​​τ​1​​τ​2​​​​(exp(−​τ​1​​​​t−t​s​​​​)−exp(−​τ​2​​​​t−t​s​​​​)) Where tst_st​s​​ denotes the spike timing of the presynatic neuron, τ1\\tau_1τ​1​​ and τ2\\tau_2τ​2​​ are time constants. While implementing with BrainPy, we use the following differential equation form, dsdt=x \t\t\\frac {ds} {dt} = x ​dt​​ds​​=x dxdt=−τ1+τ2τ1τ2x−sτ1τ2 \\frac {dx}{dt} =- \\frac{\\tau_1+\\tau_2}{\\tau_1 \\tau_2}x - \\frac s {\\tau_1 \\tau_2} ​dt​​dx​​=−​τ​1​​τ​2​​​​τ​1​​+τ​2​​​​x−​τ​1​​τ​2​​​​s​​ if (fire), then x←x+1 \\text{if (fire), then} \\ x \\leftarrow x+ 1 if (fire), then x←x+1 Here we specify the logic of increment of xxx in the update function when the presynaptic neurons fire. The code is as follows: Then we expect to see the following result: (2) Alpha synapse Dynamics of Alpha synapse is given by, s=t−tsτexp(−t−tsτ) s = \\frac{t - t_s}{\\tau} \\exp(-\\frac{t - t_s}{\\tau}) s=​τ​​t−t​s​​​​exp(−​τ​​t−t​s​​​​) As the dual exponential synapse, tst_st​s​​ denotes the spike timing of the presynaptic neuron, with a time constant τ\\tauτ. The differential equation form of alpha synapse is also very similar with the dual exponential synapses, with τ=τ1=τ2\\tau = \\tau_1 = \\tau_2τ=τ​1​​=τ​2​​, as shown below: dsdt=x \\frac {ds} {dt} = x ​dt​​ds​​=x dxdt=−2xτ−sτ2 \\frac {dx}{dt} =- \\frac{2x}{\\tau} - \\frac s {\\tau^2} ​dt​​dx​​=−​τ​​2x​​−​τ​2​​​​s​​ if (fire), then x←x+1 \\text{if (fire), then} \\ x \\leftarrow x+ 1 if (fire), then x←x+1 Code implementation is similar: Then we expect to see the following result: (3) Single exponential decay Sometimes we can ignore the rising process in modeling, and only need to model the decay process. Therefore, the formula of single exponential decay model is more simplified: dsdt=−sτdecay \\frac {ds}{dt}=-\\frac s {\\tau_{decay}} ​dt​​ds​​=−​τ​decay​​​​s​​ if (fire), then s←s+1 \\text{if (fire), then} \\ s \\leftarrow s+1 if (fire), then s←s+1 The implementing code is given by: Then we expect to see the following result: (4) Voltage jump Sometimes even the decay process can be ignored, so there is a voltage jump model, which is given by: if (fire), then s←s+1 \\text{if (fire), then} \\ s \\leftarrow s+1 if (fire), then s←s+1 The code is as follows: Then we expect to see the following result: Current-based and Conductance-based synapses So far, we have modeled the gating variable sss, now let's see how to model the effect of the gating variables on the synaptic current. The current that passes through a synaptic channel is denoted as III. There are two different methods to model the relationships between sss and III: current-based and conductance-based. The main difference between them is whether the synaptic current is influenced by the membrane potential of postsynaptic neurons. (1) Current-based The formula of the current-based model is as follow: I∝s I \\propto s I∝s While coding, we usually multiply sss by a weight www. We can implement excitatory and inhibitory synapses by adjusting the positive and negative values of the weight www. The delay of synapses is implemented by applying the delay time to the I_syn variable using the register_constant_delay function provided by BrainPy. (2) Conductance-based In the conductance-based model, the conductance is g=g¯sg=\\bar{g} sg=​g​¯​​s. Therefore, according to Ohm's law, the formula is given by: I=g¯s(V−E) I=\\bar{g}s(V-E) I=​g​¯​​s(V−E) Here EEE is a reverse potential, which can determine whether the effect of III is inhibition or excitation. For example, when the resting potential is about -65, subtracting a lower EEE, such as -75, will become positive, thus will change the direction of the current in the formula and produce the suppression current. The EEE value of excitatory synapses is relatively high, such as 0. In terms of implementation, you can apply a synaptic delay to the variable g. 2.1.2 Electrical Synapses In addition to the chemical synapses described earlier, electrical synapses are also common in our neural system. (a) (b) Fig. 2-3 (a) Gap junction connection between two cells. (b) An equivalent diagram. (Adaptive from Sterratt et al., 2011 2) As shown in the Fig. 2-3a, two neurons are connected by junction channels and can conduct electricity directly. Therefore, it can be seen that two neurons are connected by a constant resistance, as shown in the Fig. 2-3b. According to Ohm's law, the current of one neuron is given by, I1=w(V0−V1) I_{1} = w (V_{0} - V_{1}) I​1​​=w(V​0​​−V​1​​) where V0V_0V​0​​ and V1V_1V​1​​ are the membrane potentials of the two neurons, and the synaptic weight www is equivalent with the conductance. While implementing with BrainPy, you only need to specify the equation in the update function. Then we can run a simulation. import matplotlib.pyplot as plt import numpy as np neu0 = bm.neurons.LIF(2, monitors=['V'], t_refractory=0) neu0.V = np.ones(neu0.V.shape) * -10. neu1 = bm.neurons.LIF(3, monitors=['V'], t_refractory=0) neu1.V = np.ones(neu1.V.shape) * -10. syn = Gap_junction(k_spikelet=5., pre=neu0, post=neu1, conn=bp.connect.All2All()) syn.w = np.ones(syn.w.shape) * .5 net = bp.Network(neu0, neu1, syn) net.run(100., inputs=(neu0, 'input', 30.)) fig, gs = bp.visualize.get_figure(row_num=2, col_num=1, ) fig.add_subplot(gs[1, 0]) plt.plot(net.ts, neu0.mon.V[:, 0], label='V0') plt.legend() fig.add_subplot(gs[0, 0]) plt.plot(net.ts, neu1.mon.V[:, 0], label='V1') plt.legend() plt.show() References 1. Gerstner, Wulfram, et al. Neuronal dynamics: From single neurons to networks and models of cognition. Cambridge University Press, 2014. ↩ 2. Sterratt, David, et al. Principles of computational modeling in neuroscience. Cambridge University Press, 2011. ↩ "},"synapse/plasticity.html":{"url":"synapse/plasticity.html","title":"2.2 Plasticity models","keywords":"","body":"2.2 Plasticity Models We just talked about synaptic dynamics, but we haven't talked about synaptic plasticity. Next, let's see how to use BrainPy to implement synaptic plasticity. Plasticity mainly distinguishes short-term plasticity from long-term plasticity. We will first introduce short-term plasticity (STP), and then introduce several different models of long-term synaptic plasticity (also known as learning rules). The introduction is as follows: Short-term plasticity Long-term plasticity Spike-timing dependent plasticity Rate-based Hebb rule Oja's rule BCM rule 2.2.1 Short-term plasticity (STP) Let's first look at short-term plasticity. We will start with the results of the experiment. Fig. 2-1 shows the changes of the membrane potential of postsynaptic neurons as the firing of presynaptic neurons. We can see that when the presynaptic neurons repeatedly firing with short intervals, the response of the postsynaptic neurons becomes weaker and weaker, showing a short term depression. But the response recovers after a short period of time, so this plasticity is short-term. Fig. 2-1 Short-term plasticity. (Adaptive from Gerstner et al., 2014 1) Now let's turn to the model. The short term plasticity can be described by two variables, uuu and xxx. Where uuu represents the probability of neurotransmitter release, and xxx represents the residual amount of neurotransmitters. The dynamic of a synapse with short term plasticity is given by, dIdt=−Iτ \\frac {dI} {dt} = - \\frac I {\\tau} ​dt​​dI​​=−​τ​​I​​ dudt=−uτf \\frac {du} {dt} = - \\frac u {\\tau_f} ​dt​​du​​=−​τ​f​​​​u​​ dxdt=1−xτd \\frac {dx} {dt} = \\frac {1-x} {\\tau_d} ​dt​​dx​​=​τ​d​​​​1−x​​ if (pre fire), then{u+=u−+U(1−u−)I+=I−+Au+x−x+=x−−u+x− \\text{if (pre fire), then} \\begin{cases} u^+ = u^- + U(1-u^-) \\\\ I^+ = I^- + Au^+x^- \\\\ x^+ = x^- - u^+x^- \\end{cases} if (pre fire), then​⎩​⎪​⎨​⎪​⎧​​​u​+​​=u​−​​+U(1−u​−​​)​I​+​​=I​−​​+Au​+​​x​−​​​x​+​​=x​−​​−u​+​​x​−​​​​ where the dynamics of the synaptic current III can be one of the dynamics we introduced in the previous section (i.e., the dynamic of gating variable sss under current-based condition). UUU and AAA are two constants representing the increments of uuu and III after a presynaptic spike, respectively. τf\\tau_fτ​f​​ and τd\\tau_dτ​d​​ are time constants of uuu and xxx, respectively. In this model, uuu contributes to the short-term facilitation (STF) by increasing from 0 whenever there is a spike on the presynaptic neuron; while xxx contributes to the short-term depression (STD) by decreasing from 1 after the presynaptic spike. The two directions of facilitation and depression occur simultaneously, and the value of τf\\tau_fτ​f​​ and τd\\tau_dτ​d​​ determines which direction of plasticity plays a dominant role. The code implemented with BrainPy is as follows: Then let's define a function to run the code. Like synapse models, we need two neuron groups to be connected. Besides the dynamic of sss, we also want to see how uuu and xxx changes over time, so we monitor 's', 'u' and 'x' and plot them. def run_stp(**kwargs): neu1 = bm.neurons.LIF(1, monitors=['V']) neu2 = bm.neurons.LIF(1, monitors=['V']) syn = STP(pre=neu1, post=neu2, conn=bp.connect.All2All(), monitors=['s', 'u', 'x'], **kwargs) net = bp.Network(neu1, syn, neu2) net.run(100., inputs=(neu1, 'input', 28.)) # plot fig, gs = bp.visualize.get_figure(2, 1, 3, 7) fig.add_subplot(gs[0, 0]) plt.plot(net.ts, syn.mon.u[:, 0], label='u') plt.plot(net.ts, syn.mon.x[:, 0], label='x') plt.legend() fig.add_subplot(gs[1, 0]) plt.plot(net.ts, syn.mon.s[:, 0], label='s') plt.legend() plt.xlabel('Time (ms)') plt.show() Let's first set tau_d > tau_f. run_stp(U=0.2, tau_d=150., tau_f=2.) The plots show that when we set the parameters τd>τf\\tau_d > \\tau_fτ​d​​>τ​f​​, xxx recovers very slowly, and uuu decays very quickly, so in the end, the transmitter is not enough to open the receptors, showing STD dominants. Then let's set tau_f > tau_d. run_stp(U=0.1, tau_d=10, tau_f=100.) We can see from the figure that when we set τf>τd\\tau_f > \\tau_dτ​f​​>τ​d​​, on the contrary, every time xxx is used, it will be added back quickly. There are always enough transmitters available. At the same time, the decay of uuu is very slow, so the probability of releasing transmitters is getting higher and higher, showing STF dominants. 2.2.2 Long-term Plasticity Spike-timing dependent plasticity (STDP) Fig. 2-2 shows the spiking timing dependent plasticity (STDP) of experimental results. The x-axis is the time difference between the spike of the presynaptic neuron and the postsynaptic neuron. The left part of the zero represents the spike timing of the presynaptic neuron earlier than that of the postsynaptic neuron, which shows long term potentiation (LTP); and the right side of the zero represents the postsynaptic neuron fires before the presynaptic neuron does, showing long term depression (LTD). Fig. 2-2 Spike timing dependent plasticity. (Adaptive from Bi & Poo, 2001 2) The computational model of STDP is given by, dAsdt=−Asτs \\frac {dA_s} {dt} = - \\frac {A_s} {\\tau_s} ​dt​​dA​s​​​​=−​τ​s​​​​A​s​​​​ dAtdt=−Atτt \\frac {dA_t} {dt} = - \\frac {A_t} {\\tau_t} ​dt​​dA​t​​​​=−​τ​t​​​​A​t​​​​ if (pre fire), then{s←s+wAs←As+ΔAsw←w−At \\text{if (pre fire), then} \\begin{cases} s \\leftarrow s + w \\\\ A_s \\leftarrow A_s + \\Delta A_s \\\\ w \\leftarrow w - A_t \\end{cases} if (pre fire), then​⎩​⎪​⎨​⎪​⎧​​​s←s+w​A​s​​←A​s​​+ΔA​s​​​w←w−A​t​​​​ if (post fire), then{At←At+ΔAtw←w+As \\text{if (post fire), then} \\begin{cases} A_t \\leftarrow A_t + \\Delta A_t \\\\ w \\leftarrow w + A_s \\end{cases} if (post fire), then{​A​t​​←A​t​​+ΔA​t​​​w←w+A​s​​​​ Where www is the synaptic weight, and sss is the same gating variable as we mentioned in the previous section. Like the STP model, LTD and LTP are controlled by two variables AsA_{s}A​s​​ and AtA_{t}A​t​​, respectively. ΔAs\\Delta A_sΔA​s​​ and ΔAt\\Delta A_tΔA​t​​ are the increments of AsA_{s}A​s​​ and AtA_{t}A​t​​, respectively. τs\\tau_sτ​s​​ and τt\\tau_tτ​t​​ are time constants. According to this model, when a presynaptic neuron fire before the postsynaptic neuron, AsA_sA​s​​ increases everytime when there is a spike on the presynaptic neuron, and AtA_tA​t​​ will stay on 0 until the postsynaptic neuron fire, so www will not change for the time being. When there is a spike in the postsynaptic neuron, the increment of www will be an amount of As−AtA_s - A_tA​s​​−A​t​​, since As>AtA_s > A_tA​s​​>A​t​​ in this situation, LTP will be presented, and vice verse. Now let's see how to use BrainPy to implement this model. Here we use the single exponential decay model to implement the dynamics of sss. We control the spike timing by varying the input current of the presynaptic group and postsynaptic group. We apply the first input to the presynaptic group starting at t=5mst=5mst=5ms (with amplitude of 30 μA\\mu AμA, lasts for 15 ms to ensure to induce a spike with LIF neuron model), then start to stimulate the postsynaptic group at t=10mst=10mst=10ms. The intervals between each two inputs are 15ms15ms15ms. We keep those tpost=tpre+5t_{post}=t_{pre}+5t​post​​=t​pre​​+5 during the first 3 spike-pairs. Then we set a long interval before switching the stimulating order to be tpost=tpre−3t_{post}=t_{pre}-3t​post​​=t​pre​​−3 since the 4th spike. duration = 300. (I_pre, _) = bp.inputs.constant_current([(0, 5), (30, 15), # pre at 5ms (0, 15), (30, 15), (0, 15), (30, 15), (0, 98), (30, 15), # switch order: t_interval=98ms (0, 15), (30, 15), (0, 15), (30, 15), (0, duration-155-98)]) (I_post, _) = bp.inputs.constant_current([(0, 10), (30, 15), # post at 10 (0, 15), (30, 15), (0, 15), (30, 15), (0, 90), (30, 15), # switch order: t_interval=98-8=90(ms) (0, 15), (30, 15), (0, 15), (30, 15), (0, duration-160-90)]) Then let's run the simulation. pre = bm.neurons.LIF(1, monitors=['spike']) post = bm.neurons.LIF(1, monitors=['spike']) syn = STDP(pre=pre, post=post, conn=bp.connect.All2All(), monitors=['s', 'w']) net = bp.Network(pre, syn, post) net.run(duration, inputs=[(pre, 'input', I_pre), (post, 'input', I_post)]) # plot fig, gs = bp.visualize.get_figure(4, 1, 2, 7) def hide_spines(my_ax): plt.legend() plt.xticks([]) plt.yticks([]) my_ax.spines['left'].set_visible(False) my_ax.spines['right'].set_visible(False) my_ax.spines['bottom'].set_visible(False) my_ax.spines['top'].set_visible(False) ax=fig.add_subplot(gs[0, 0]) plt.plot(net.ts, syn.mon.s[:, 0], label=\"s\") hide_spines(ax) ax1=fig.add_subplot(gs[1, 0]) plt.plot(net.ts, pre.mon.spike[:, 0], label=\"pre spike\") plt.ylim(0, 2) hide_spines(ax1) plt.legend(loc = 'center right') ax2=fig.add_subplot(gs[2, 0]) plt.plot(net.ts, post.mon.spike[:, 0], label=\"post spike\") plt.ylim(-1, 1) hide_spines(ax2) ax3=fig.add_subplot(gs[3, 0]) plt.plot(net.ts, syn.mon.w[:, 0], label=\"w\") plt.legend() # hide spines plt.yticks([]) ax3.spines['left'].set_visible(False) ax3.spines['right'].set_visible(False) ax3.spines['top'].set_visible(False) plt.xlabel('Time (ms)') plt.show() The simulation result shows that weights www increase when the presynaptic neuron fire before the postsynaptic neuron (before 150ms); and decrease when the order switched (after 150ms). Oja's rule Next, let's look at the rate model based on Hebbian learning. Because Hebbian learning is \"fire together, wire together\", regardless of the order before and after, spiking time can be ignored, so it can be simplified as a rate-based model. Let's first look at the general form of Hebbian learning. For the jjj to iii connection, rj,rir_j, r_ir​j​​,r​i​​ denotes the firing rate of pre- and post-neuron groups, respectively. According to the locality characteristic of Hebbian learning, The change of wijw_{ij}w​ij​​ is affected by www itself and rj,rir_j, r_ir​j​​,r​i​​, we get the following differential equation. ddtwij=F(wij;ri,rj) \\frac d {dt} w_{ij} = F(w_{ij}; r_{i},r_j) ​dt​​d​​w​ij​​=F(w​ij​​;r​i​​,r​j​​) The following formula is obtained by Taylor expansion on the right side of the above formula. ddtwij=c00wij+c10wijrj+c01wijri+c20wijrj2+c02wijri2+c11wijrirj+O(r3) \\frac d {dt} w_{ij} = c_{00} w_{ij} + c_{10} w_{ij} r_j + c_{01} w_{ij} r_i + c_{20} w_{ij} r_j ^2 + c_{02} w_{ij} r_i ^2 + c_{11} w_{ij} r_i r_j + O(r^3) ​dt​​d​​w​ij​​=c​00​​w​ij​​+c​10​​w​ij​​r​j​​+c​01​​w​ij​​r​i​​+c​20​​w​ij​​r​j​2​​+c​02​​w​ij​​r​i​2​​+c​11​​w​ij​​r​i​​r​j​​+O(r​3​​) The 6th term contains rirjr_i r_jr​i​​r​j​​，only if c11c_{11}c​11​​ is not zero can the \"fire together\" of Hebbian learning be satisfied. For example, the formula of Oja's rule is as follows, which corresponds to the 5th and 6th terms of the above formula. ddtwij=γ[rirj−wijri2] \\frac d {dt} w_{ij} = \\gamma [r_i r_j - w_{ij} r_i ^2 ] ​dt​​d​​w​ij​​=γ[r​i​​r​j​​−w​ij​​r​i​2​​] γ\\gammaγ represents the learning rate. Now let's see how to use BrainPy to implement Oja's rule. Since Oja's rule is a rate-based model, we need a rate-based neuron model to see this learning rule of two groups of neurons. Fig. 2-3 Connection of neuron groups. We aim to implement the connection as shown in Fig. 2-3. The purple neuron group receives inputs from the blue and red groups. The external input to the post group is exactly the same as the red one, while the blue one is the same at first, but not later. The simulation code is as follows. It can be seen from the results that at the beginning, when the two groups of neurons were given input at the same time, their weights increased simultaneously, and the response of post became stronger and stronger, showing LTP. After 100ms, the blue group is no longer fire together, only the red group still fire together, and only the weights of the red group are increased. The results accord with the \"fire together, wire together\" of Hebbian learning. BCM rule Now let's see other example of Hebbian learning, the BCM rule. It's given by, ddtwij=ηri(ri−rθ)rj \\frac d{dt} w_{ij} = \\eta r_i(r_i - r_\\theta) r_j ​dt​​d​​w​ij​​=ηr​i​​(r​i​​−r​θ​​)r​j​​ where η\\etaη represents the learning rate, and rθr_\\thetar​θ​​ represents the threshold of learning (see Fig. 2-4). Fig. 2-4 shows the right side of the formula. When the firing rate is greater than the threshold, there is LTP, and when the firing rate is lower than the threshold, there is LTD. Therefore, the selectivity can be achieved by adjusting the threshold rθr_\\thetar​θ​​. Fig. 2-4 BCM rule (Adaptive from Gerstner et al., 2014 1) We will implement the same connections as the previous Oja's rule (Fig. 2-3), with different firing rates. Here the two groups of neurons are alternately firing. Among them, the blue group is always stronger than the red one. We adjust the threshold by setting it as the time average of rir_ir​i​​, that is rθ=f(ri)r_\\theta = f(r_i)r​θ​​=f(r​i​​). The code implemented by BrainPy is as follows. Then we can run the simulation with the following code. n_post = 1 n_pre = 20 # group selection group1, duration = bp.inputs.constant_current(([1.5, 1], [0, 1]) * 20) group2, duration = bp.inputs.constant_current(([0, 1], [1., 1]) * 20) group1 = bp.ops.vstack(((group1,)*10)) group2 = bp.ops.vstack(((group2,)*10)) input_r = bp.ops.vstack((group1, group2)) pre = neu(n_pre, monitors=['r']) post = neu(n_post, monitors=['r']) bcm = BCM(pre=pre, post=post,conn=bp.connect.All2All(), monitors=['w']) net = bp.Network(pre, bcm, post) net.run(duration, inputs=(pre, 'r', input_r.T, \"=\")) w1 = bp.ops.mean(bcm.mon.w[:, :10, 0], 1) w2 = bp.ops.mean(bcm.mon.w[:, 10:, 0], 1) r1 = bp.ops.mean(pre.mon.r[:, :10], 1) r2 = bp.ops.mean(pre.mon.r[:, 10:], 1) fig, gs = bp.visualize.get_figure(2, 1, 3, 12) fig.add_subplot(gs[1, 0], xlim=(0, duration), ylim=(0, w_max)) plt.plot(net.ts, w1, 'b', label='w1') plt.plot(net.ts, w2, 'r', label='w2') plt.title(\"weights\") plt.ylabel(\"weights\") plt.xlabel(\"t\") plt.legend() fig.add_subplot(gs[0, 0], xlim=(0, duration)) plt.plot(net.ts, r1, 'b', label='r1') plt.plot(net.ts, r2, 'r', label='r2') plt.title(\"inputs\") plt.ylabel(\"firing rate\") plt.xlabel(\"t\") plt.legend() plt.show() The results show that the blue group with stronger input demonstrating LTP, while the red group with weaker input showing LTD, so the blue group is being chosen. References 1. Gerstner, Wulfram, et al. Neuronal dynamics: From single neurons to networks and models of cognition. Cambridge University Press, 2014. ↩ 2. Bi, Guo-qiang, and Mu-ming Poo. \"Synaptic modification by correlated activity: Hebb's postulate revisited.\" Annual review of neuroscience 24.1 (2001): 139-166. ↩ "},"networks.html":{"url":"networks.html","title":"3. Network models","keywords":"","body":"3. Network models With the neuron and synapse models we have realized with BrainPy, users can now build networks of there own. In this section, we will introduce two main types of network models as examples: 1) spiking neural networks that model and compute each neuron or synapse separately; 2) firing rate networks that simplify neuron groups in the network as firing rate units and compute each neuron group as one unit. 3.1 Spiking Neural Networks 3.2 Firing Rate Networks "},"networks/spiking_neural_networks.html":{"url":"networks/spiking_neural_networks.html","title":"3.1 Spiking neural networks","keywords":"","body":"3.1 Spiking Neural Network 3.1.1 E/I balanced network In 1990s, biologists found in experiments that neuron activities in brain cortex show a temporal irregular spiking pattern. This pattern exists widely in brain areas, but researchers knew few about its mechanism or function. Vreeswijk and Sompolinsky (1996) proposed E/I balanced network to explain this irregular spiking pattern. The feature of this network is the strong, random and sparse synapse connections between neurons. Because of this feature and corresponding parameter settings, each neuron in the network will receive great excitatory and inhibitory input from within the network. However, these two types of inputs will cancel each other, and maintain the total internal input at a relatively small order of magnitude, which is only enough to generate action potentials. The randomness and noise in E/I balanced network give each neuron in the network an internal input which varies with time and space at the order of threshold potential. Therefore, the firing of neurons also has randomness, ensures that E/I balanced network can generate temporal irregular firing pattern spontaneously. Fig.3-1 Structure of E/I balanced network | Vreeswijk and Sompolinsky, 1996 Vreeswijk and Sompolinsky also suggested a possible function of this irregular firing pattern: E/I balanced network can respond to the changes of external stimulus quickly. As shown in Fig. 3-3, when there is no external input, the distribution of neurons’ membrane potentials in E/I balanced network follows a relatively uniform random distribution between resting potential V0V_0V​0​​and threshold potential θ\\thetaθ. Fig.3-2 Distribution of neuron membrane potentials in E/I balanced network | Tian et al., 2020 When we give the network a small constant external stimulus, those neurons whose membrane potentials fall near the threshold potential will soon meet the threshold, therefore spike rapidly. On the network scale, the firing rate of the network can adjust rapidly once the input changes. Simulation suggests that the delay of network response to input and the delay of synapses have the same time scale, and both are significantly smaller than the delay of a single neuron from resting potential to generating a spike. So E/I balanced network may provide a fast response mechanism for neural networks. Fig. 3-1 shows the structure of E/I balanced network: 1) Neurons: Neurons are realized with LIF neuron model. The neurons can be divided into excitatory neurons and inhibitory neurons, the ratio of the two types of neurons is NEN_EN​E​​: NIN_IN​I​​ = 4:1. 2) Synapses: Synapses are realized with exponential synapse model. 4 groups of synapse connections are generated between the two groups of neurons, that is, excitatory-excitatory connection (E2E conn), excitatory-inhibitory connection (E2I conn), inhibitory-excitatory connection (I2E conn) and inhibitory-inhibitory connection (I2I conn). For excitatory or inhibitory synapse connections, we define synapse weights with different signal. 3) Inputs: All neurons in the network receive a constant external input current. See above section 1 and 2 for definition of LIF neuron and exponential synapse. After simulation, we visualize the raster plot and firing rate-t plot of E/I balanced network. the network firing rate changes from strong synchronization to irregular fluctuation. Fig.3-3 E/I balanced net raster plot 3.1.2 Decision Making Network The modeling of computational neuroscience networks can correspond to specific physiological tasks. For example, in the visual motion discrimination task (Roitman and Shadlen, 2002), rhesus watch a video in which random dots move towards left or right with definite coherence. Rhesus are required to choose the direction that most dots move to and give their answer by saccade. At the meantime, researchers record the activity of their LIP neurons by implanted electrode. Fig.3-4 Experimental Diagram Wang (2002) proposed a decision making network to model the activity of rhesus LIP neurons during decision making period in the visual motion discrimination task. As shown in Fig. 3-5, this network is based on E/I balanced network. The ratio of excitatory neurons and inhibitory neurons is NE:NI=4:1N_E:N_I = 4:1N​E​​:N​I​​=4:1, and parameters are adjusted to maintain the balanced state. To accomplish the decision making task, among the excitatory neuron group, two selective subgroup A and B are chosen, both with a size of NA=NB=0.15NEN_A = N_B = 0.15N_EN​A​​=N​B​​=0.15N​E​​. These two subgroups are marked as A and B in Fig. 3-5, and we call other excitatory neurons as non-selective neurons, Nnon=(1−2∗0.15)NEN_{non} = (1-2*0.15)N_EN​non​​=(1−2∗0.15)N​E​​. Fig.3-5 structure of decision makingnetwork As it is in E/I balanced network, 4 groups of synapses ---- E2E connection, E2I connection, I2E connection and I2I connection ---- are built in decision making network. Excitatory connections are realized with AMPA synapse, inhibitory connections are realized with GABAa synapse. Decision making network needs to make a decision among the two choice, i.e., among the two subgroups A and B in this task. To achieve this, network must discriminate between these two groups. Excitatory neurons in the same subgroup should self-activate, and inhibit neurons in another selective subgroup. Therefore, E2E connections are structured in the network. As shown in Sheet 3-1, w+>1>w−w+ > 1 > w-w+>1>w−. In this way, a relative activation is established within the subgroups by stronger excitatory synapse connections, and relative inhibition is established between two subgroups or between selective and non-selective subgroups by weaker excitatory synapse connections. Sheet 3-1 Weight of synapse connections between E-neurons We give two types of external inputs to the decision making network: 1) Background inputs from other brain areas without specific meaning. Represented as high frequency Poisson input mediated by AMPA synapse. 2) Stimulus inputs from outside the brain, which are given only to the two selective subgroup A and B. Represented as lower frequency Poisson input mediated by AMPA synapse. The frequency of Poisson input given to A and B subgroup have a certain difference, simulate the difference in the number of dots moving to left and right in physiological experiments, induce the network to make a decision among these two subgroups. ρA=ρB=μ0/100 \\rho_A = \\rho_B = \\mu_0/100 ρ​A​​=ρ​B​​=μ​0​​/100 μA=μ0+ρA∗c \\mu_A = \\mu_0 + \\rho_A * c μ​A​​=μ​0​​+ρ​A​​∗c μB=μ0+ρB∗c \\mu_B = \\mu_0 + \\rho_B * c μ​B​​=μ​0​​+ρ​B​​∗c Every 50ms, the Poisson frequencies fxf_xf​x​​ change once, follows a Gaussian distribution defined by mean μx\\mu_xμ​x​​ and variance δ2\\delta^2δ​2​​. fA∼N(μA,δ2) f_A \\sim N(\\mu_A, \\delta^2) f​A​​∼N(μ​A​​,δ​2​​) fB∼N(μB,δ2) f_B \\sim N(\\mu_B, \\delta^2) f​B​​∼N(μ​B​​,δ​2​​) During the simulation, subgroup A receives a larger stimulus input than B, after a definite delay period, the activity of group A is significantly higher than group B, which means, the network chooses the right direction. Fig.3-6 decision making network "},"networks/rate_models.html":{"url":"networks/rate_models.html","title":"3.2 Firing rate networks","keywords":"","body":"3.2 Firing rate networks 3.2.1 Decision model In addition to spiking models, BrainPy can also implement Firing rate models. Let's first look at the implementation of a simplified version of the decision model. The model was simplified by the researcher (Wong & Wang, 2006)1 through a series of means such as mean field approach. In the end, there are only two variables, S1S_1S​1​​ and S2S_2S​2​​, which respectively represent the state of two neuron groups and correspond to two options. Fig. 3-1 Reduced decision model. (Adaptive from Wong & Wang, 2006 1) The model is given by, dS1dt=−S1τ+(1−S1)γr1 \\frac{dS_1} {dt} = -\\frac {S_1} \\tau + (1-S_1) \\gamma r_1 ​dt​​dS​1​​​​=−​τ​​S​1​​​​+(1−S​1​​)γr​1​​ dS2dt=−S2τ+(1−S2)γr2 \\frac{dS_2} {dt} = -\\frac {S_2} \\tau + (1-S_2) \\gamma r_2 ​dt​​dS​2​​​​=−​τ​​S​2​​​​+(1−S​2​​)γr​2​​ where r1r_1 r​1​​ and r2r_2r​2​​ is the firing rate of two neuron groups, which is given by the input-output function, ri=f(Isyn,i) r_i = f(I_{syn, i}) r​i​​=f(I​syn,i​​) f(I)=aI−b1−exp[−d(aI−b)] f(I)= \\frac {aI-b} {1- \\exp [-d(aI-b)]} f(I)=​1−exp[−d(aI−b)]​​aI−b​​ where Isyn,iI_{syn, i}I​syn,i​​ is given by the model structure (Fig. 3-1), Isyn,1=J11S1−J12S2+I0+I1 I_{syn, 1} = J_{11} S_1 - J_{12} S_2 + I_0 + I_1 I​syn,1​​=J​11​​S​1​​−J​12​​S​2​​+I​0​​+I​1​​ Isyn,2=J22S2−J21S1+I0+I2 I_{syn, 2} = J_{22} S_2 - J_{21} S_1 + I_0 + I_2 I​syn,2​​=J​22​​S​2​​−J​21​​S​1​​+I​0​​+I​2​​ where I0I_0I​0​​ is the background current, and the external inputs I1,I2I_1, I_2I​1​​,I​2​​ are determined by the total input strength μ0\\mu_0μ​0​​ and a coherence c′c'c​′​​. The higher the coherence, the more definite S1S_1S​1​​ is the correct answer, while the lower the coherence, the more random it is. The formula is as follows: I1=JA, extμ0(1+c′100%) I_1 = J_{\\text{A, ext}} \\mu_0 (1+\\frac {c'}{100\\%}) I​1​​=J​A, ext​​μ​0​​(1+​100%​​c​′​​​​) I2=JA, extμ0(1−c′100%) I_2 = J_{\\text{A, ext}} \\mu_0 (1-\\frac {c'}{100\\%}) I​2​​=J​A, ext​​μ​0​​(1−​100%​​c​′​​​​) The code implementation is as follows: we can create a neuron group class, and use S1S_1S​1​​ and S2S_2S​2​​ to store the two states of the neuron group. The dynamics of the model can be implemented by a derivative function for dynamics analysis. Then we can define a function to perform phase plane analysis. Let's first look at the case when there is no external input. At this time, μ0=0\\mu_0 = 0μ​0​​=0. phase_analyze(I=0., coh=0.) Output: plot nullcline ... plot fixed point ... Fixed point #1 at s2=0.06176109215560733, s1=0.061761097890810475 is a stable node. Fixed point #2 at s2=0.029354239100062428, s1=0.18815448592736211 is a saddle node. Fixed point #3 at s2=0.0042468423702408655, s1=0.6303045696241589 is a stable node. Fixed point #4 at s2=0.6303045696241589, s1=0.004246842370235128 is a stable node. Fixed point #5 at s2=0.18815439944520335, s1=0.029354240536530615 is a saddle node. plot vector field ... It can be seen that it is very convenient to use BrainPy for dynamics analysis. The vector field and fixed point indicate which option will fall in the end under different initial values. Here, the x-axis is S2S_2S​2​​ which represents choice 2, and the y-axis is S1S_1S​1​​, which represents choice 1. As you can see, the upper-left fixed point represents choice 1, the lower-right fixed point represents choice 2, and the lower-left fixed point represents no choice. Now let's see which option will eventually fall under different initial values with different coherence, and we fix the external input strength to 30. Now let's look at the phase plane under different coherences when we fix the external input strength to 30. # coherence = 0% print(\"coherence = 0%\") phase_analyze(I=30., coh=0.) # coherence = 51.2% print(\"coherence = 51.2%\") phase_analyze(I=30., coh=0.512) # coherence = 100% print(\"coherence = 100%\") phase_analyze(I=30., coh=1.) coherence = 0% plot nullcline ... plot fixed point ... Fixed point #1 at s2=0.6993504413889349, s1=0.011622049526766405 is a stable node. Fixed point #2 at s2=0.49867489858358865, s1=0.49867489858358865 is a saddle node. Fixed point #3 at s2=0.011622051540013889, s1=0.6993504355529329 is a stable node. plot vector field ... coherence = 51.2% plot nullcline ... plot fixed point ... Fixed point #1 at s2=0.5673124813731691, s1=0.2864701069327971 is a saddle node. Fixed point #2 at s2=0.6655747347157656, s1=0.027835279565912054 is a stable node. Fixed point #3 at s2=0.005397687847426814, s1=0.7231453520305031 is a stable node. plot vector field ... coherence = 100% plot nullcline ... plot fixed point ... Fixed point #1 at s2=0.0026865954387078755, s1=0.7410985604497689 is a stable node. plot vector field ... 3.2.2 CANN Let's see another example of firing rate model, a continuous attractor neural network (CANN)2. Fig. 3-2 demonstrates the structure of one-dimensional CANN. Fig. 3-2 Structure of CANN. (Adaptive from Wu et al., 2008 2) We denote (x) as the parameter space site of the neuron group, and the dynamics of the total synaptic input of neuron group (x) u(x)u(x)u(x) is given by: τdu(x,t)dt=−u(x,t)+ρ∫dx′J(x,x′)r(x′,t)+Iext \\tau \\frac{du(x,t)}{dt} = -u(x,t) + \\rho \\int dx' J(x,x') r(x',t)+I_{ext} τ​dt​​du(x,t)​​=−u(x,t)+ρ∫dx​′​​J(x,x​′​​)r(x​′​​,t)+I​ext​​ Where r(x′,t)r(x', t)r(x​′​​,t) is the firing rate of the neuron group (x'), which is given by: r(x,t)=u(x,t)21+kρ∫dx′u(x′,t)2 r(x,t) = \\frac{u(x,t)^2}{1 + k \\rho \\int dx' u(x',t)^2} r(x,t)=​1+kρ∫dx​′​​u(x​′​​,t)​2​​​​u(x,t)​2​​​​ The intensity of excitatory connection between (x) and (x') J(x,x′)J(x, x')J(x,x​′​​) is given by a Gaussian function: J(x,x′)=12πaexp(−∣x−x′∣22a2) J(x,x') = \\frac{1}{\\sqrt{2\\pi}a}\\exp(-\\frac{|x-x'|^2}{2a^2}) J(x,x​′​​)=​√​2π​​​a​​1​​exp(−​2a​2​​​​∣x−x​′​​∣​2​​​​) The external input IextI_{ext}I​ext​​ is related to position z(t)z(t)z(t): Iext=Aexp[−∣x−z(t)∣24a2] I_{ext} = A\\exp\\left[-\\frac{|x-z(t)|^2}{4a^2}\\right] I​ext​​=Aexp[−​4a​2​​​​∣x−z(t)∣​2​​​​] While implementing with BrainPy, we create a class of CANN1D by inheriting bp.NeuGroup, and initialize it with __init__ function like neuron models. Then we define the functions. Where the functions dist and make_conn are designed to get the connection strength JJJ between each of the two neuron groups. In the make_conn function, we first calculate the distance matrix between each of the two xxx. Because neurons are arranged in rings, the value of xxx is between −π-\\pi−π and π\\piπ, so the range of ∣x−x′∣|x-x'|∣x−x​′​​∣ is 2π2\\pi2π, and -π\\piπ and π\\piπ are the same points (the actual maximum value is π\\piπ, that is, half of z_range, the distance exceeded needs to be subtracted from a z_range). We use the dist function to handle the distance on the ring. The get_stimulus_by_pos function processes external inputs based on position pos, which allows users to get input current by setting target positions. For example, in a simple population coding, we give an external input of pos=0, and we run in this way: cann = CANN1D(num=512, k=0.1, monitors=['u']) I1 = cann.get_stimulus_by_pos(0.) Iext, duration = bp.inputs.constant_current([(0., 1.), (I1, 8.), (0., 8.)]) cann.run(duration=duration, inputs=('input', Iext)) Then lets plot an animation by calling the bp.visualize.animate_1D function. def plot_animate(frame_step=5, frame_delay=50): bp.visualize.animate_1D(dynamical_vars=[{'ys': cann.mon.u, 'xs': cann.x, 'legend': 'u'}, {'ys': Iext, 'xs': cann.x, 'legend': 'Iext'}], frame_step=frame_step, frame_delay=frame_delay, show=True) plot_animate(frame_step=1, frame_delay=100) We can see that the shape of uuu encodes the shape of external input. Now we add random noise to the external input to see how the shape of uuu changes. cann = CANN1D(num=512, k=8.1, monitors=['u']) dur1, dur2, dur3 = 10., 30., 0. num1 = int(dur1 / bp.backend.get_dt()) num2 = int(dur2 / bp.backend.get_dt()) num3 = int(dur3 / bp.backend.get_dt()) Iext = np.zeros((num1 + num2 + num3,) + cann.size) Iext[:num1] = cann.get_stimulus_by_pos(0.5) Iext[num1:num1 + num2] = cann.get_stimulus_by_pos(0.) Iext[num1:num1 + num2] += 0.1 * cann.A * np.random.randn(num2, *cann.size) cann.run(duration=dur1 + dur2 + dur3, inputs=('input', Iext)) plot_animate() We can see that the shape of uuu remains like a bell shape, which indicates that it can perform template matching based on the input. Now let's give a moving input, we vary the position of the input with np.linspace, we will see that the uuu will follow the input, i.e., smooth tracking. cann = CANN1D(num=512, k=8.1, monitors=['u']) dur1, dur2, dur3 = 20., 20., 20. num1 = int(dur1 / bp.backend.get_dt()) num2 = int(dur2 / bp.backend.get_dt()) num3 = int(dur3 / bp.backend.get_dt()) position = np.zeros(num1 + num2 + num3) position[num1: num1 + num2] = np.linspace(0., 12., num2) position[num1 + num2:] = 12. position = position.reshape((-1, 1)) Iext = cann.get_stimulus_by_pos(position) cann.run(duration=dur1 + dur2 + dur3, inputs=('input', Iext)) plot_animate() Reference 1. Wong, K.-F. & Wang, X.-J. A Recurrent Network Mechanism of Time Integration in Perceptual Decisions. J. Neurosci. 26, 1314–1328 (2006). ↩ 2. Si Wu, Kosuke Hamaguchi, and Shun-ichi Amari. \"Dynamics and computation of continuous attractors.\" Neural computation 20.4 (2008): 994-1025. ↩ "}}