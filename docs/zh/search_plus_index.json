{"./":{"url":"./","title":"0. 简介","keywords":"","body":"BrainPy介绍 在本章中，我们将介绍计算神经科学中的一系列神经元模型、突触模型和网络模型。在正式开始之前，我们希望先为读者简单介绍如何使用BrainPy实现计算神经科学模型，以方便读者理解附在每个模型之后的BrainPy实现代码。 BrainPy是一个用于计算神经科学和类脑计算的Python平台。要使用BrainPy进行建模，用户通常需要完成以下三个步骤： 1）为神经元和突触模型定义Python类。BrainPy预先定义了数种基类，用户在实现特定模型时，只需继承相应的基类，并在模型的Python类中定义特定的方法来告知BrainPy该模型在仿真的每个时刻所需的操作。在此过程中，BrainPy在微分方程（如ODE、SDE等）的数值积分、多种后端（如Numpy、PyTorch等）适配等功能上辅助用户，简化实现的代码逻辑。 2）将模型的Python类实例化为代表神经元群或突触群的对象，将这些对象传入到BrainPy的Network类的构造函数中，初始化一个网络，并调用run方法进行仿真。 3）调用BrainPy的测度模块measure或可视化模块visualize等，展示仿真结果。 带着上述对BrainPy的粗略理解，我们希望下述各节中的代码实例能够帮助读者更好地理解计算神经科学模型和其中蕴含的思想。下面，我们将按照神经元模型, 突触模型, and 网络模型的顺序进行介绍。 关于BrainPy的更多细节请参考我们的Github仓库：https://github.com/PKU-NIP-Lab/BrainPy和https://github.com/PKU-NIP-Lab/BrainModels。 "},"neurons.html":{"url":"neurons.html","title":"1. 神经元模型","keywords":"","body":"1. 神经元模型 按照从繁到简的顺序，我们可以将神经元模型主要分为三类：生理模型、简化模型和发放率模型。 1.1 生物背景 1.2 生理模型 1.3 简化模型 1.4 发放率模型 "},"neurons/biological_background.html":{"url":"neurons/biological_background.html","title":"1.1 生物背景","keywords":"","body":"1.1 生物背景 作为神经系统的基本单位，神经元曾经在很长的一段时间内，对研究者保持着神秘。但在最近的几个世纪，随着实验技术的发展，研究者已为这些在我们神经系统中无休无止地工作的小东西画出了一张基本的肖像。 要想用计算神经科学的方法建模神经元，我们必须从这张真实细胞膜的肖像入手。 Fig. 1-1 Neuron membrane diagram | what-when-how.com 上图是一张带有离子通道和磷脂双层膜的神经元膜的一般性示意图。细胞膜将离子和液体划分为胞内和胞外两侧，部分地阻止了胞内和胞外的物质交换。于是产生了膜电位——即，细胞膜两侧的电位差。 细胞内或细胞外的一个离子主要受两种力的支配：细胞内外离子浓度差产生的扩散力和细胞内外电位差产生的电场力。当这两种力达到平衡时，离子的总受力为零，每种离子都达到其自身的离子平衡电位。与此同时，神经元的膜电位维持在一个小于零的值。 这个由所有离子平衡电位整合而成的膜电位，就被称为静息电位，神经元则在此时进入所谓的静息状态。如果神经元不受外部干扰，则它将自发寻找到平衡的静息状态，并维持在这一状态。 然而，从外部输入到内部的循环输入，从刺激输入到背景的噪声输入，每一毫秒，神经系统都接收到不计其数的外部扰动。面对这些输入，神经元发放动作电位（或峰电位）来在神经系统中处理、传递信息。 Fig. 1-2 Action Potential | Wikipedia 如果要具体地解释一个动作电位，则我们说在疏水性磷脂双层膜两侧的离子穿过如图1-1所示的离子通道进行交换。由于比如说，外界输入引起的环境变化，离子通道会在打开和关闭的状态之间切换，控制着相应离子的交换速率。 在状态切换时，膜两侧特定离子的浓度（主要是Na+和K+的浓度）发生变化，引发膜电位的剧变：膜电位先上升到一个峰值，随后在短时间内迅速跌回一个小于静息电位的值。生物上，当膜电位发生这样的一系列变化时，我们说神经元产生了动作电位，或峰电位，或说神经元激活。 一个动作电位基本可以被分为三个阶段，去极化、复极化和不应期。在去极化阶段，钠离子流入细胞，钾离子流出细胞，但钠离子的流入速度更快，因此膜电位从一个低的静息电位开始缓慢升高，这种流入和流出速度之间的差值也逐渐增大，在膜电位高于阈值电位后，其骤增到某一大于0的正值。随后钾离子的流出加快，钠离子的流入减缓，流出速度变得大于流入速度，膜电位从而降低，复极化到一个常常低于静息电位的值。此后，由于相对更低的膜电位以及离子通道的失活，神经元在短时间内立刻产生另一个动作电位的概率极小，这种情况将一直维持到我们称作不应期的这段时间结束。 单个动作电位已经非常复杂，但在我们的神经系统中，仅仅一个神经元就可以在一秒之内产生多个动作电位。这些动作电位是以什么样的模式被产生的？不同类型的神经元可能在面对不同的输入时发放动作电位，并且它们发放的方式可以被分为数种放电模式，图1-3中画出了其中一部分。 Figure 1-3 Some firing patterns 计算神经科学在细胞层面上所要建模的，正是动作电位的形状和这些放电模式。 "},"neurons/biophysical_models.html":{"url":"neurons/biophysical_models.html","title":"1.2 生理模型","keywords":"","body":"1.2 生理模型 1.2.1 Hodgkin-Huxley模型 Hodgkin和Huxley（1952）在枪乌贼的巨轴突上用膜片钳技术记录了动作电位的产生，并提出了经典的神经元模型Hodgkin-Huxley模型（HH模型）。 上一节我们已经介绍了细胞膜的一般性模板。Hodgkin和Huxley将神经元细胞膜建模为等效电路，如下图所示。 Fig. 1-4 Equivalent circuit diagram | NeuroDynamics 图1-4中所示的，是将图1-1中真实神经元膜转换为电子元件所得到的等效电路图。在图1-4中，电容CCC表示电导率很低的疏水性磷脂双层膜，电流III表示外界刺激。 由于钠离子通道和钾离子通道在动作电位的形成中非常重要，这两个离子通道被单独建模为电路图右侧所示的两个并联的可变电阻RNaR_{Na}R​Na​​和RKR_KR​K​​，而电阻RRR代表膜上所有非特定的离子通道。电源 ENaE_{Na}E​Na​​, EKE_KE​K​​ 和ELE_LE​L​​对应着由相应离子的浓度差所引起的电位差。 考虑基尔霍夫第一定律，即，对于电路中的任一点，流入该点的总电流和流出该点的总电流相等，图1-4可被建模为如下所示的微分方程： CdVdt=−(g¯Nam3h(V−ENa)+g¯Kn4(V−EK)+gleak(V−Eleak))+I(t) C \\frac{dV}{dt} = -(\\bar{g}_{Na} m^3 h (V - E_{Na}) + \\bar{g}_K n^4(V - E_K) + g_{leak}(V - E_{leak})) + I(t) C​dt​​dV​​=−(​g​¯​​​Na​​m​3​​h(V−E​Na​​)+​g​¯​​​K​​n​4​​(V−E​K​​)+g​leak​​(V−E​leak​​))+I(t) dxdt=αx(1−x)−βx,x∈{Na,K,leak} \\frac{dx}{dt} = \\alpha_x(1-x) - \\beta_x , x \\in \\{ Na, K, leak \\} ​dt​​dx​​=α​x​​(1−x)−β​x​​,x∈{Na,K,leak} 这就是HH模型。注意在如上的第1个方程中，右侧的前三项分别代表穿过钠离子通道，钾离子通道和其他非特定离子通道的电流，同时I(t)I(t)I(t)表示一个外部输入。在方程左侧，CdVdt=dQdt=IC\\frac{dV}{dt} = \\frac{dQ}{dt} = IC​dt​​dV​​=​dt​​dQ​​=I是穿过电容的电流。 在计算经过离子通道的电流时，除了欧姆定律I=U/R=gUI = U/R = gUI=U/R=gU之外，HH模型还引入了三个门控变量m、n和h来控制离子通道的打开/关闭状态。准确的说，变量m和h控制着钠离子通道的状态，变量n控制着钾离子通道的状态，并且，一个离子通道的真实电导是其最大电导g¯\\bar{g}​g​¯​​和通道门控变量状态的乘积。 门控变量的动力学可以被表示为一种类马尔可夫的形式，其中αx\\alpha_xα​x​​代表门控变量xxx的激活速率，而βx\\beta_xβ​x​​代表xxx的失活速率。αx\\alpha_xα​x​​和βx\\beta_xβ​x​​的公式（如下所示）是由实验数据拟合得到的。 αm(V)=0.1(V+40)1−exp(−(V+40)10) \\alpha_m(V) = \\frac{0.1(V+40)}{1 - exp(\\frac{-(V+40)}{10})} α​m​​(V)=​1−exp(​10​​−(V+40)​​)​​0.1(V+40)​​ βm(V)=4.0exp(−(V+65)18) \\beta_m(V) = 4.0 exp(\\frac{-(V+65)}{18}) β​m​​(V)=4.0exp(​18​​−(V+65)​​) αh(V)=0.07exp(−(V+65)20) \\alpha_h(V) = 0.07 exp(\\frac{-(V+65)}{20}) α​h​​(V)=0.07exp(​20​​−(V+65)​​) βh(V)=11+exp(−(V+35)10) \\beta_h(V) = \\frac{1}{1 + exp(\\frac{-(V + 35)}{10})} β​h​​(V)=​1+exp(​10​​−(V+35)​​)​​1​​ αn(V)=0.01(V+55)1−exp(−(V+55)10) \\alpha_n(V) = \\frac{0.01(V+55)}{1 - exp(\\frac{-(V+55)}{10})} α​n​​(V)=​1−exp(​10​​−(V+55)​​)​​0.01(V+55)​​ βn(V)=0.125exp(−(V+65)80) \\beta_n(V) = 0.125 exp(\\frac{-(V+65)}{80}) β​n​​(V)=0.125exp(​80​​−(V+65)​​) 在我们的github仓库中运行代码:：https://github.com/PKU-NIP-Lab/BrainModels（如无特殊说明，下同） BrainPy仿真的HH模型的V-t图如下所示。真实神经元产生动作电位的三个阶段，去极化、复极化和不应期都可以对应到下图中。另外在去极化时，HH模型的膜电位先是缓慢积累外部输入，一旦其值高于某个特定值，膜电位就转为快速增长，这也复现了真实动作电位的形状。 "},"neurons/reduced_models.html":{"url":"neurons/reduced_models.html","title":"1.3 简化模型","keywords":"","body":"1.3 简化模型 启发自生理实验的Hodgkin-Huxley模型准确但昂贵。研究者们提出了简化模型，希望能降低仿真的运行时间和计算资源的消耗。 These models are simple and easy to compute, while they can still reproduce the main pattern of neuron behaviors. Although their representation capabilities are not as good as biophysical models, such a loss of accuracy is sometimes acceptable considering their simplicity. 简化模型简单、易于计算，并且他们仍然能够复现神经元发放的主要特征。尽管它们的表示能力不如生理模型，但和它们的简便相比，在特定场景下研究者们有时也可以接受一定的精度损失。 1.3.1 泄漏积分-发放模型 最经典的简化模型，莫过于Lapicque（1907）提出的泄漏积分-发放模型（Leaky Integrate-and-Fire model, LIF model）。LIF模型是由微分方程表示的积分过程和由条件判断表示的发放过程的结合： τdVdt=−(V−Vrest)+RI(t) \\tau\\frac{dV}{dt} = - (V - V_{rest}) + R I(t) τ​dt​​dV​​=−(V−V​rest​​)+RI(t) If V>VthV > V_{th}V>V​th​​, neuron fires, V←Vreset V \\gets V_{reset} V←V​reset​​ τ=RC\\tau = RCτ=RC是LIF模型的时间常数，τ\\tauτ越大，模型的动力学就越慢。如上所示的方程对应于一个比HH模型的等效电路图更加简单的等效电路，因为它不再建模钠离子通道和钾离子通道。实际上，LIF模型中只有电阻RRR，电容CCC，电源EEE和外部输入III被建模。 Fig1-4 Equivalent circuit of LIF model 比起HH模型，LIF模型没有建模动作电位的形状，也就是说，在发放一个峰电位之前，LIF神经元的膜电位不会骤增。并且在原始模型中，不应期也被忽视了。为了仿真模拟不应期，必须再补充一个条件判断： 如果 t−tlastspike=refractoryperiod t-t_{last spike}t−t​lastspike​​=refractoryperiod 则神经元处在不应期中，膜电位VVV不再更新。 1.3.2 二次积分-发放模型 为了追求更强的表示能力，Latham等人（2000）提出了二次积分-发放模型（Quadratic Integrate-and-Fire model，QuaIF model），他们在微分方程的右侧添加了一个二阶项，使得神经元能产生更好的动作电位。 τdVdt=a0(V−Vrest)(V−Vc)+RI(t) \\tau\\frac{d V}{d t}=a_0(V-V_{rest})(V-V_c) + RI(t) τ​dt​​dV​​=a​0​​(V−V​rest​​)(V−V​c​​)+RI(t) 在上式中，a0a_0a​0​​是控制着膜电位发放前的斜率的参数，VcV_cV​c​​是动作电位初始化的临界值。当低于 VCV_CV​C​​时，膜电位 VVV缓慢增长，一旦越过 VCV_CV​C​​， VVV就转为迅速增长。 1.3.3 指数积分-发放模型 指数积分发放模型（Exponential Integrate-and-Fire model, ExpIF model）（Fourcaud-Trocme et al., 2003）的表示能力比QuaIF模型更强。ExpIF模型在微分方程右侧增加了指数项，使得模型现在可以产生更加真实的动作电位。 τdVdt=−(V−Vrest)+ΔTeV−VTΔT+RI(t) \\tau \\frac{dV}{dt} = - (V - V_{rest}) + \\Delta_T e^{\\frac{V - V_T}{\\Delta_T}} + R I(t) τ​dt​​dV​​=−(V−V​rest​​)+Δ​T​​e​​Δ​T​​​​V−V​T​​​​​​+RI(t) 在指数项中VTV_TV​T​​是动作电位初始化的临界值，在其下VVV缓慢增长，其上VVV迅速增长。ΔT\\Delta_TΔ​T​​是ExpIF模型中动作电位的斜率。当ΔT→0\\Delta_T\\to 0Δ​T​​→0时，ExpIF模型中动作电位的形状将等同于Vth=VTV_{th} = V_TV​th​​=V​T​​的LIF模型（Fourcaud-Trocme et al.，2003）。 1.3.4 适应性指数积分-发放模型 当面对恒定的外部刺激时，神经元一开始高频发放，随后发放率逐渐降低，最终稳定在一个较小值，这种现象生物上称为适应。 为了复现神经元的适应行为，研究者们在已有的积分-发放模型，如LIF、QuaIF和ExpIF模型上增加了权重变量w。这里我们介绍其中一个经典模型，适应性指数积分-发放模型（Adaptive Exponential Integrate-and-Fire model，AdExIF model）（Gerstner et al.，2014）。 τmdVdt=−(V−Vrest)+ΔTeV−VTΔT−Rw+RI(t) \\tau_m \\frac{dV}{dt} = - (V - V_{rest}) + \\Delta_T e^{\\frac{V - V_T}{\\Delta_T}} - R w + R I(t) τ​m​​​dt​​dV​​=−(V−V​rest​​)+Δ​T​​e​​Δ​T​​​​V−V​T​​​​​​−Rw+RI(t) τwdwdt=a(V−Vrest)−w+bτw∑δ(t−tf)) \\tau_w \\frac{dw}{dt} = a(V - V_{rest})- w + b \\tau_w \\sum \\delta(t - t^f)) τ​w​​​dt​​dw​​=a(V−V​rest​​)−w+bτ​w​​∑δ(t−t​f​​)) 就如它的名字所示，AdExIF模型的第一个微分方程和我们上面介绍的ExpIF模型非常相似，不同的是适应项，即方程中−Rw-Rw−Rw这一项。 权重项www受到第二个微分方程的调控。aaa描述了权重变量www对VVV的下阈值波动的敏感性，bbb表示www在一次发放后的增长值，并且www也会随时间衰减。 给神经元一个恒定输入，在连续发放几个动作电位之后，www的值将会上升到一个高点，减慢VVV的增长速度，从而降低神经元的发放率。 1.3.5 Hindmarsh-Rose模型 为了模拟神经元中的爆发式发放（bursting，即在短时间内的连续发放），Hindmarsh和Rose（1984）提出了Hindmarsh-Rose模型，引入了第三个模型变量zzz作为慢变量来控制神经元的爆发。 dVdt=y−aV3+bV2−z+I \\frac{d V}{d t} = y - a V^3 + b V^2 - z + I ​dt​​dV​​=y−aV​3​​+bV​2​​−z+I dydt=c−dV2−y \\frac{d y}{d t} = c - d V^2 - y ​dt​​dy​​=c−dV​2​​−y dzdt=r(s(V−Vrest)−z) \\frac{d z}{d t} = r (s (V - V_{rest}) - z) ​dt​​dz​​=r(s(V−V​rest​​)−z) The VVV variable refers to membrane potential, and yyy, zzz are two gating variables. The parameter bbb in dVdt\\frac{dV}{dt}​dt​​dV​​ equation allows the model to switch between spiking and bursting states, and controls the spiking frequency. rrr controls slow variable zzz's variation speed, affects the number of spikes per burst when bursting, and governs the spiking frequency together with bbb. The parameter sss governs adaptation, and other parameters are fitted by firing patterns. 变量VVV表示膜电位，yyy和zzz是两个门控变量。在dV/dtdV/dtdV/dt方程中的参数bbb允许模型在发放和爆发两个状态之间切换，并且控制着发放的频率。参数rrr控制着慢变量zzz的变化速度，影响着神经元爆发式发放时，每次爆发包含的动作电位个数，并且和bbb一起统筹控制发放频率，参数sss控制着适应行为。其它参数根据发放模式拟合得到。 在下图中，画出了三个变量随时间的变化，可以看到慢变量zzz的改变要慢于VVV和yyy。而且，VVV和yyy在仿真过程中呈周期性变化。 利用BrainPy的理论分析模块analysis，我们可以分析出这种周期性的产生原因。在模型的相图中，VVV和yyy的轨迹趋近于一个极限环，因此他们的值会沿着极限环发生周期性的改变。 1.3.6 归纳积分-发放模型 归纳积分-发放模型（Generalized Integrate-and-Fire model，GeneralizedIF model）（Mihalaş et al.，2009）整合了多种发放模式。该模型拥有四个模型变量，能产生多于20种发放模式，并可以通过调整参数在各模式之间切换。 dIjdt=−kjIj,j=1,2 \\frac{d I_j}{d t} = - k_j I_j, j = {1, 2} ​dt​​dI​j​​​​=−k​j​​I​j​​,j=1,2 τdVdt=(−(V−Vrest)+R∑jIj+RI) \\tau \\frac{d V}{d t} = ( - (V - V_{rest}) + R\\sum_{j}I_j + RI) τ​dt​​dV​​=(−(V−V​rest​​)+R​j​∑​​I​j​​+RI) dVthdt=a(V−Vrest)−b(Vth−Vth∞) \\frac{d V_{th}}{d t} = a(V - V_{rest}) - b(V_{th} - V_{th\\infty}) ​dt​​dV​th​​​​=a(V−V​rest​​)−b(V​th​​−V​th∞​​) 当VVV达到VthV_{th}V​th​​时，GeneralizedIF模型发放： Ij←RjIj+Aj I_j \\leftarrow R_j I_j + A_j I​j​​←R​j​​I​j​​+A​j​​ V←Vreset V \\leftarrow V_{reset} V←V​reset​​ Vth←max(Vthreset,Vth) V_{th} \\leftarrow max(V_{th_{reset}}, V_{th}) V​th​​←max(V​th​reset​​​​,V​th​​) 在dV/dtdV/dtdV/dt的方程中，和所有积分-发放模型一样，τ\\tauτ表示时间常数，VVV表示膜电位，VrestV_{rest}V​rest​​表示静息电位，RRR为电阻，而III为外部输入。 不过，在GIF模型中，数目可变的内部电流被加入到方程中，写作∑jIj\\sum_j I_j∑​j​​I​j​​一项。每一个IjI_jI​j​​都代表神经元中的一个内部电流，并以速率kjk_jk​j​​衰减。RjR_jR​j​​和AjA_jA​j​​是自由参数，RjR_jR​j​​描述了IjI_jI​j​​的重置值对发放前的IjI_jI​j​​的值的依赖，AjA_jA​j​​是在发放后加到IjI_jI​j​​上的一个常数值。 可变的阈值电位VthV_{th}V​th​​受两个参数的调控：aaa 描述了VthV_{th}V​th​​对膜电位VVV 的依赖，bbb描述了VthV_{th}V​th​​接近阈值电位在时间趋近于无穷大时的值Vth∞V_{th_{\\infty}}V​th​∞​​​​的速率。VthresetV_{th_{reset}}V​th​reset​​​​是当神经元发放时，阈值电位被重置到的值。 "},"neurons/firing_rate_models.html":{"url":"neurons/firing_rate_models.html","title":"1.4 发放率模型","keywords":"","body":"1.4 发放率模型 发放率模型比简化模型更加简单。在这些模型中，每个计算单元代表一个神经元群，而单神经元模型中的膜电位变量VVV也被发放率变量aaa（或rrr或ν\\nuν）所取代。下面我们将介绍一个经典的发放率单元。 1.4.1 发放率单元 Wilson和Cowan（1972）来表示在兴奋性和抑制性皮层神经元微柱（？）中的活动。变量aea_ea​e​​和aia_ia​i​​中的每个元素都表示一个包含复数神经元的皮层微柱中神经元群的平均活动水平。 τedae(t)dt=−ae(t)+(ke−re∗ae(t))∗S(c1ae(t)−c2ai(t)+Iexte(t)) \\tau_e \\frac{d a_e(t)}{d t} = - a_e(t) + (k_e - r_e * a_e(t)) * \\mathcal{S}(c_1 a_e(t) - c_2 a_i(t) + I_{ext_e}(t)) τ​e​​​dt​​da​e​​(t)​​=−a​e​​(t)+(k​e​​−r​e​​∗a​e​​(t))∗S(c​1​​a​e​​(t)−c​2​​a​i​​(t)+I​ext​e​​​​(t)) τidai(t)dt=−ai(t)+(ki−ri∗ai(t))∗S(c3ae(t)−c4ai(t)+Iexti(t)) \\tau_i \\frac{d a_i(t)}{d t} = - a_i(t) + (k_i - r_i * a_i(t)) * \\mathcal{S}(c_3 a_e(t) - c_4 a_i(t) + I_{ext_i}(t)) τ​i​​​dt​​da​i​​(t)​​=−a​i​​(t)+(k​i​​−r​i​​∗a​i​​(t))∗S(c​3​​a​e​​(t)−c​4​​a​i​​(t)+I​ext​i​​​​(t)) S(input)=11+exp(−a(input−θ))−11+exp(aθ) \\mathcal{S}(input) = \\frac{1}{1 + exp(- a(input - \\theta))} - \\frac{1}{1 + exp(a\\theta)} S(input)=​1+exp(−a(input−θ))​​1​​−​1+exp(aθ)​​1​​ 下标x∈{e,i}x\\in\\{e, i\\}x∈{e,i}表示该参数或变量对应着兴奋性还是抑制性的神经元群。在微分方程中，τx\\tau_xτ​x​​表示神经元群的时间常数，参数，kxk_xk​x​​和rxr_xr​x​​共同控制不应期，axa_xa​x​​和θx\\theta_xθ​x​​分别代表Sigmoid函数S(input)\\mathcal{S}(input)S(input)的斜率和相位参数，且兴奋性和抑制性的神经元群分别收到外界输入IextxI_{ext_{x}}I​ext​x​​​​。 "},"synapses.html":{"url":"synapses.html","title":"2. 突触模型","keywords":"","body":"2. 突触模型 当我们建模了神经元的动作电位后，我们需要建立突触模型来描述神经元之间的信息传递过程。突触把神经元连接起来，使不同神经元得以沟通，对于组成神经网络也是至关重要的。 在本章中，我们将在2.1节介绍包括化学突触与电突触的模型，同时涵盖从简单到复杂的各种突触动力学模型。 突触模型另一个重要的地方在于突触可塑性的实现，包括突触短时程与长时程可塑性，对于学习、记忆与神经网络的计算、训练都非常重要，我们将会在2.2节中介绍突触可塑性的部分。 2.1 突触模型 2.2 可塑性模型 "},"synapse/dynamics.html":{"url":"synapse/dynamics.html","title":"2.1 突触动力学模型","keywords":"","body":"2.1 突触模型 我们在前面的章节中已经学习了如何建模神经元的动作电位，那么神经元之间是怎么连接起来的呢？神经元的动作电位是如何在不同神经元之间传导的呢？这里，我们将介绍如何用BrainPy对神经元之间的沟通进行模拟仿真。 2.1.1 化学突触 生物背景 我们可以从图2-1这个生物突触的图中看到神经元之间信息传递的过程。当突触前神经元的动作电位传递到轴突的末端（terminal），它会往突触间隙释放神经递质（又称递质）。神经递质会和突触后神经元上的受体结合，从而引起突触后神经元膜电位的改变，这种改变成为突触后电位（PSP）。根据神经递质种类的不同，突触后电位可以是兴奋性或是抑制的。例如谷氨酸（Glutamate）就是一种重要兴奋性的神经递质，而GABA则是一种重要的抑制性神经递质。 神经递质与受体的结合可能会导致离子通道的打开（离子型受体）或改变化学反应的过程（代谢型受体）。 在本节中，我们将介绍如何使用BrainPy来实现一些常见的突触模型，主要有： AMPA和NMDA：它们都是谷氨酸的离子型受体，被结合后都可以直接打开离子通道。但是NMDA通常会被镁离子（Mg2+^{2+}​2+​​）堵住，无法对谷氨酸做出反应。由于镁离子对电压敏感，当AMPA导致突触后电位改变到超过镁离子的阈值以后，镁离子就会离开NMDA通道，让NMDA可以对谷氨酸做出反应。因此，NMDA的反应是比较慢的。 GABAA和GABAB：它们是GABA的两类受体，其中GABAA是离子型受体，通常可以产生快速的抑制性电位；而GABAB则为代谢型受体，通常会产生缓慢的抑制性电位。 图 2-1 生物突触 (引自 Gerstner et al., 2014 1) 为了简便地建模从神经递质释放到引起突触后电位的这个过程，我们可以把神经递质释放、递质与受体结合、受体引起的变化这些过程概括为突触前神经元的动作电位变化如何引起突触后神经元膜上的离子通道变化，即用门控变量sss来描述每当突触前神经元产生动作电位的时候，有多少比例的离子通道会被打开。我们首先来看看AMPA的例子。 AMPA模型 如前所述，AMPA（a-氨基-3-羟基-5-甲基-4-异恶唑丙酸）受体是一种离子型受体，也就是说，当它被神经递质结合后会立即打开离子通道，从而引起突触后神经元膜电位的变化。 我们可以用马尔可夫过程来描述离子通道的开关。如图2-2所示，sss代表通道打开的概率，1−s1-s1−s代表离子通道关闭的概率，α\\alphaα和β\\betaβ是转移概率（transition probability）。由于神经递质能让离子通道打开，所以从1−s1-s1−s到sss的转移概率受神经递质浓度（以[T]表示）影响。 Fig. 2-2 离子通道动力学的马尔可夫过程 把该过程用微分方程描述，得到以下式子。 dsdt=α[T](1−s)−βs \\frac {ds}{dt} = \\alpha [T] (1-s) - \\beta s ​dt​​ds​​=α[T](1−s)−βs 其中，α[T]\\alpha [T]α[T] 表示从状态(1−s)(1-s)(1−s)到状态(s)(s)(s)的转移概率；β\\betaβ 表示从sss到(1−s)(1-s)(1−s)的转移概率。 下面我们来看看如何用BrainPy去实现这样一个模型。首先，我们要定义一个类，因为突触是连接两个神经元的，所以这个类继承自bp.TwoEndConn。在这个类中，和神经元模型一样，我们用一个derivative函数来实现上述微分方程，并在后面的__init__函数中初始化这个函数，指定用bp.odeint来解这个方程，并指定数值积分方法。由于这微分方程是线性的，我们选用exponential_euler方法。 首先，在突触中，我们需要pre和post来分别表示这个突触所连接的突触前神经元与突触后神经元。需要注意的是，pre和post都是向量，代表两群神经元，因此，我们还需要指定两群神经元具体链接情况的conn。在这里，我们可以从conn中获得连接矩阵conn_mat。 然后我们在update函数中更新sss。 我们已经定义好了一个AMPA类，现在可以画出sss随时间变化的图了。我们首先写一个run_syn函数来方便之后运行更多的突触模型，然后把AMPA类和需要自定义的变量传入这个函数来运行并画图。 运行以上代码，我们就会看到以下的结果： 由上图可以看出，当突触前神经元产生一个动作电位，sss的值会先增加，然后衰减。 NMDA模型 如前所述，NMDA受体一开始被镁离子堵住，而随着膜电位的变化，镁离子又会移开，我们用cMgc_{Mg}c​Mg​​表示镁离子的浓度，它对突触后膜的电导ggg的影响可以由以下公式描述： g∞=(1+e−αV⋅cMgβ)−1 g_{\\infty} =(1+{e}^{-\\alpha V} \\cdot \\frac{c_{Mg} } {\\beta})^{-1} g​∞​​=(1+e​−αV​​⋅​β​​c​Mg​​​​)​−1​​ g=g¯⋅g∞s g = \\bar{g} \\cdot g_{\\infty} s g=​g​¯​​⋅g​∞​​s 在此公式中，g∞g_{\\infty}g​∞​​的值随着镁离子浓度增加而减小。而随着电压VVV增加，g∞g_{\\infty}g​∞​​越来越不受镁离子的影响，建模了镁离子随电压增加而离开的效应。α,β\\alpha, \\betaα,β和g¯\\bar{g}​g​¯​​是一些常数。门控变量sss和AMPA模型类似，其动力学由以下公式给出： dsdt=−sτdecay+ax(1−s) \\frac{d s}{dt} =-\\frac{s}{\\tau_{\\text{decay}}}+a x(1-s) ​dt​​ds​​=−​τ​decay​​​​s​​+ax(1−s) dxdt=−xτrise \\frac{d x}{dt} =-\\frac{x}{\\tau_{\\text{rise}}} ​dt​​dx​​=−​τ​rise​​​​x​​ if (pre fire), then x←x+1 \\text{if (pre fire), then} \\ x \\leftarrow x+ 1 if (pre fire), then x←x+1 其中，τdecay\\tau_{\\text{decay}}τ​decay​​和τrise\\tau_{\\text{rise}}τ​rise​​分别为sss衰减及上升的时间常数，aaa是参数。 接下来我们用BrainPy来实现NMDA模型，代码如下。 由于前面我们已经定义了run_syn函数，在这里我们可以直接调用： run_syn(NMDA) 由图可以看出，NMDA的上升和衰减过程都比AMPA模型更加缓慢。 GABAB模型 GABAB是一种代谢型受体，神经递质和受体结合后不会直接打开离子通道，而是通过G蛋白作为第二信使来起作用。因此，这里我们用[R][R][R]表示多少比例的受体被激活，并用[G][G][G]表示激活的G蛋白的浓度，sss由[G][G][G]调节，公式如下： d[R]dt=k3[T](1−[R])−k4[R] \\frac{d[R]}{dt} = k_3 [T](1-[R])- k_4 [R] ​dt​​d[R]​​=k​3​​[T](1−[R])−k​4​​[R] d[G]dt=k1[R]−k2[G] \\frac{d[G]}{dt} = k_1 [R]- k_2 [G] ​dt​​d[G]​​=k​1​​[R]−k​2​​[G] s=[G]4[G]4+Kd s =\\frac{[G]^{4}} {[G]^{4}+K_{d}} s=​[G]​4​​+K​d​​​​[G]​4​​​​ [R][R][R]的动力学类似于AMPA模型中sss，受神经递质浓度[T][T][T]影响，k3,k4k_3, k_4k​3​​,k​4​​表示转移概率。[G][G][G]的动力学受[R][R][R]影响，并由参数k1,k2k_1, k_2k​1​​,k​2​​控制。KdK_dK​d​​为一个常数。 用BrainPy实现的代码如下。 （抑制性 -> E, I） 指数及Alpha模型 由于许多突触模型都有类似AMPA突触那样先上升后下降的动力学特征，有时候我们建模不需要具体对应到生物学上的突触，因此，有人提出了一些抽象的突触模型。这里，我们会介绍四种这类抽象模型在BrainPy上的实现。这些模型在Brain-Models中也有现成的提供。 (1) 双指数差（Differences of two exponentials） 我们首先来看双指数差（Differences of two exponentials）模型，它有两个指数项相减，公式如下： s=τ1τ2τ1−τ2(exp(−t−tsτ1)−exp(−t−tsτ2)) s = \\frac {\\tau_1 \\tau_2}{\\tau_1 - \\tau_2} (\\exp(-\\frac{t - t_s}{\\tau_1}) - \\exp(-\\frac{t - t_s}{\\tau_2})) s=​τ​1​​−τ​2​​​​τ​1​​τ​2​​​​(exp(−​τ​1​​​​t−t​s​​​​)−exp(−​τ​2​​​​t−t​s​​​​)) 其中 tst_st​s​​ 表示突触前神经元产生动作电位的时间，τ1\\tau_1τ​1​​和τ2\\tau_2τ​2​​为时间常数。 在BrainPy的实现中，我们采用以下微分方程形式： dsdt=x \t\t\\frac {ds} {dt} = x ​dt​​ds​​=x dxdt=−τ1+τ2τ1τ2x−sτ1τ2 \\frac {dx}{dt} =- \\frac{\\tau_1+\\tau_2}{\\tau_1 \\tau_2}x - \\frac s {\\tau_1 \\tau_2} ​dt​​dx​​=−​τ​1​​τ​2​​​​τ​1​​+τ​2​​​​x−​τ​1​​τ​2​​​​s​​ if (fire), then x←x+1 \\text{if (fire), then} \\ x \\leftarrow x+ 1 if (fire), then x←x+1 这里我们用update函数来控制xxx增加的逻辑。代码如下： (2) Alpha突触 Alpha突触的动力学由以下公式给出： s=t−tsτexp(−t−tsτ) s = \\frac{t - t_s}{\\tau} \\exp(-\\frac{t - t_s}{\\tau}) s=​τ​​t−t​s​​​​exp(−​τ​​t−t​s​​​​) 和双指数差模型类似， tst_st​s​​ 表示突触前神经元产生动作电位的时间，不同的是这里只有一个时间常数τ\\tauτ。微分方程形式如下： dsdt=x \\frac {ds} {dt} = x ​dt​​ds​​=x dxdt=−2xτ−sτ2 \\frac {dx}{dt} =- \\frac{2x}{\\tau} - \\frac s {\\tau^2} ​dt​​dx​​=−​τ​​2x​​−​τ​2​​​​s​​ if (fire), then x←x+1 \\text{if (fire), then} \\ x \\leftarrow x+ 1 if (fire), then x←x+1 可以看出alpha模型和双指数差模型其实很相似，相当于是τ=τ1=τ2\\tau=\\tau_1 = \\tau_2τ=τ​1​​=τ​2​​。因此，代码实现上也很接近： (3) 单指数衰减（Single exponential decay） 下面我们来介绍一种更加简化的模型，它忽略了上升的过程，而只建模了衰减（decay）的过程。单指数衰减（Single exponential decay）模型用一个指数项来描述衰减的过程，公式如下： dsdt=−sτdecay \\frac {ds}{dt}=-\\frac s {\\tau_{decay}} ​dt​​ds​​=−​τ​decay​​​​s​​ if (fire), then s←s+1 \\text{if (fire), then} \\ s \\leftarrow s+1 if (fire), then s←s+1 代码实现如下： (4) 电压跳变（Voltage jump） 电压跳变（Voltage jump）模型比单指数衰减模型还要更加简化，它连衰退的过程也忽略了，公式如下： if (fire), then s←s+1 \\text{if (fire), then} \\ s \\leftarrow s+1 if (fire), then s←s+1 在实现上，只需要在update函数中更新sss即可。代码如下： 基于电流的和基于电导的突触 目前为止，我们都在介绍门控变量sss的动力学，现在让我们来看看突触电流如何变化。我们用III表示通过突触的电流，从sss到III的关系有有两种不同的方法来建模，分别为 基于电流（current-based） 与基于电导（conductance-based）。两者的主要区别在于突触电流是否受突触后神经元膜电位的影响。 （1）基于电流（Current-based） 基于电流的模型公式如下： I∝s I \\propto s I∝s 在代码实现上，我们通常会乘上一个权重www。我们可以通过调整权重www的正负值来实现兴奋性和抑制性突触。另外，我们通过使用BrainPy提供的register_constant_delay函数给变量I_ syn加上延迟时间来实现突触的延迟。 （2）基于电导（Conductance-based） 在基于电导的模型中，电导为g=g¯sg=\\bar{g}sg=​g​¯​​s。因此，根据欧姆定律得公式如下： I=g¯s(V−E) I=\\bar{g}s(V-E) I=​g​¯​​s(V−E) 这里EEE是一个反转电位（reverse potential），它可以决定III的方向是抑制还是兴奋。例如，当静息电位约为-65时，减去比它更低的EEE，例如-75，将变为正，从而改变公式中电流的方向并产生抑制电流。兴奋性突触的EEE一般为比较高的值，如0。 代码实现上，可以把延迟时间应用到变量g上。 2.1.2 电突触 除了前面介绍的化学突触以外，电突触在我们神经系统中也很常见。 (a) (b) Fig. 2-3 (a) 神经元间的缝隙连接. (b) 等效模型. (修改自 Sterratt et al., 2011 2) 如图2-3a所示，两个神经元通过连接通道（junction channels）相连，可以直接导电，这种连接又称为缝隙连接（gap junction）。因此，可以看作是两个神经元由一个常数电阻连起来，如图2-3b所示。 根据欧姆定律可得以下公式： I1=w(V0−V1) I_{1} = w (V_{0} - V_{1}) I​1​​=w(V​0​​−V​1​​) 这里V0V_0V​0​​和V1V_1V​1​​分别为两个神经元的膜电位，突触权重www表示常数电导。 在BrainPy的实现中，只需要在update函数里更新即可。 定义好了缝隙连接的类以后，我们跑模拟来看给0号神经元输入时，1号神经元的电位变化。我们首先实例化两个LIF神经元模型，并用缝隙连接把它们连接起来。然后仅给0号神经元neu0一个恒定的电流，neu1没有外界输入。 import matplotlib.pyplot as plt import numpy as np neu0 = bm.neurons.LIF(2, monitors=['V'], t_refractory=0) neu0.V = np.ones(neu0.V.shape) * -10. neu1 = bm.neurons.LIF(3, monitors=['V'], t_refractory=0) neu1.V = np.ones(neu1.V.shape) * -10. syn = Gap_junction(pre=neu0, post=neu1, conn=bp.connect.All2All(), k_spikelet=5.) syn.w = np.ones(syn.w.shape) * .5 net = bp.Network(neu0, neu1, syn) net.run(100., inputs=(neu0, 'input', 30.)) fig, gs = bp.visualize.get_figure(row_num=2, col_num=1, ) fig.add_subplot(gs[1, 0]) plt.plot(net.ts, neu0.mon.V[:, 0], label='V0') plt.legend() fig.add_subplot(gs[0, 0]) plt.plot(net.ts, neu1.mon.V[:, 0], label='V1') plt.legend() plt.show() 结果图中，下图V0V_0V​0​​表示0号神经元的膜电位变化，而上图V1V_1V​1​​为1号神经元的膜电位。可以看到，当V0V_0V​0​​在阈值以下上升时，V1V_1V​1​​也跟着上升；而当V0V_0V​0​​达到阈值产生动作电位时，V1V_1V​1​​有一个快速的上升（spikelet）以后马上降到一个更低的值。 参考资料 1. Gerstner, Wulfram, et al. Neuronal dynamics: From single neurons to networks and models of cognition. Cambridge University Press, 2014. ↩ 2. Sterratt, David, et al. Principles of computational modeling in neuroscience. Cambridge University Press, 2011. ↩ "},"synapse/plasticity.html":{"url":"synapse/plasticity.html","title":"2.2 突触可塑性模型","keywords":"","body":"2.2 突触可塑性 在前一节中，我们讨论了突触动力学，但还没有涉及到突触可塑性。接下来我们将在本节中介绍如何使用BrainPy来实现突触可塑性。 可塑性主要分为短时程可塑性（short-term plasticity）与长时程可塑性（long-term plasticity）。我们将首先介绍突触短时程可塑性，然后介绍几种不同的突触长时程可塑性模型。 2.2.1 突触短时程可塑性（STP） 我们首先从实验结果来介绍突触短时程可塑性。在图2-1中，上图表示突触前神经元的动作电位，下图为突触后神经元的膜电位。我们可以看到，当突触前神经元在短时间内持续发放的时候，突触后神经元的反应越来越弱，呈现出短时程抑制 (short term depression)。而当突触前神经元停止发放几百毫秒后，再来一个动作电位，此时突触后神经元的反应基本恢复到一开始的状态，因此这个抑制效果持续的时间很短，称为短时程可塑性。 图2-1 突触短时程可塑性 (改编自 Gerstner et al., 2014 1) 那么接下来就让我们来看看描述短时称可塑性的计算模型。短时程可塑性主要由神经递质释放的概率uuu和神经递质的剩余量xxx两个变量来描述。整体的动力学方程如下： dIdt=−Iτ \\frac {dI} {dt} = - \\frac I {\\tau} ​dt​​dI​​=−​τ​​I​​ dudt=−uτf \\frac {du} {dt} = - \\frac u {\\tau_f} ​dt​​du​​=−​τ​f​​​​u​​ dxdt=1−xτd \\frac {dx} {dt} = \\frac {1-x} {\\tau_d} ​dt​​dx​​=​τ​d​​​​1−x​​ if (pre fire), then{u+=u−+U(1−u−)I+=I−+Au+x−x+=x−−u+x− \\text{if (pre fire), then} \\begin{cases} u^+ = u^- + U(1-u^-) \\\\ I^+ = I^- + Au^+x^- \\\\ x^+ = x^- - u^+x^- \\end{cases} if (pre fire), then​⎩​⎪​⎨​⎪​⎧​​​u​+​​=u​−​​+U(1−u​−​​)​I​+​​=I​−​​+Au​+​​x​−​​​x​+​​=x​−​​−u​+​​x​−​​​​ 其中，突触电流III的动力学可以采用上一节介绍的任意一种sss的动力学模型，这里我们采用简单、常用的单指数衰减（single exponential decay）模型来描述。UUU和AAA分别为uuu和III的增量，而τf\\tau_fτ​f​​和τd\\tau_dτ​d​​则分别为uuu和xxx的时间常数。 在该模型中，uuu主要贡献了短时程易化（Short-term facilitation；STF），它的初始值为0，并随着突触前神经元的每次发放而增加；而xxx则主要贡献短时程抑制（Short-term depression；STD），它的初始值为1，并在每次突触前神经元发放时都会被用掉一些（即减少）。易化和抑制两个方向是同时发生的，因此τf\\tau_fτ​f​​和τd\\tau_dτ​d​​的大小关系决定了可塑性的哪个方向起主导作用。 用BrainPy实现的代码如下： 由于突触可塑性也是发生在突触上的，这里和突触模型一样，继承自bp.TwoEndConn 定义好STP的类以后，接下来让我们来定义跑模拟的函数。跟突触模型一样，我们需要实例化两个神经元群并把它们连接在一起。结果画图方面，除了sss的动力学以外，我们也希望看到uuu和xxx随时间的变化，因此我们制定monitors=['s', 'u', 'x']。 def run_stp(**kwargs): neu1 = bm.neurons.LIF(1, monitors=['V']) neu2 = bm.neurons.LIF(1, monitors=['V']) syn = STP(pre=neu1, post=neu2, conn=bp.connect.All2All(), monitors=['s', 'u', 'x'], **kwargs) net = bp.Network(neu1, syn, neu2) net.run(100., inputs=(neu1, 'input', 28.)) # plot fig, gs = bp.visualize.get_figure(2, 1, 3, 7) fig.add_subplot(gs[0, 0]) plt.plot(net.ts, syn.mon.u[:, 0], label='u') plt.plot(net.ts, syn.mon.x[:, 0], label='x') plt.legend() fig.add_subplot(gs[1, 0]) plt.plot(net.ts, syn.mon.s[:, 0], label='s') plt.legend() plt.xlabel('Time (ms)') plt.show() 接下来，我们设 tau_d > tau_f，让我们来看看结果。 run_stp(U=0.2, tau_d=150., tau_f=2.) 从结果图中，我们可以看出当设置 τd>τf\\tau_d > \\tau_fτ​d​​>τ​f​​时，xxx每次用掉以后恢复得很慢，而uuu每次增加后很快又衰减下去了，因此从sss随时间变化的图中我们可以看到STD占主导。 接下来看看当我们设置tau_f > tau_d时的结果。 run_stp(U=0.1, tau_d=10, tau_f=100.) 结果图显示，当τf>τd\\tau_f > \\tau_dτ​f​​>τ​d​​时，xxx每次用掉后很快又补充回去了，这表示突触前神经元总是有足够的神经递质可用。同时，uuu的衰减非常缓慢，即释放神经递质的概率越来越高，从sss的动力学可以看出STF占主导地位。 2.2.2 突触长时程可塑性 脉冲时间依赖可塑性（STDP） 图2-2显示了实验上观察到的脉冲时间依赖可塑性（spiking timing dependent plasticity；STDP）的现象。x轴为突触前神经元和突触后神经元产生脉冲（spike）的时间差，位于零点左侧的数据点为突触前神经元先于突触后神经元发放的情况，由图可见此时突触权重为正，表现出长时程增强 (long term potentiation；LTP）的现象；而零点右侧则是突触后神经元比突触前神经元更先发放的情况，表现出长时程抑制 (long term depression；LTD）。 图2-2 脉冲时间依赖可塑性 (改编自 Bi & Poo, 2001 2) STDP的计算模型如下： dAsdt=−Asτs \\frac {dA_s} {dt} = - \\frac {A_s} {\\tau_s} ​dt​​dA​s​​​​=−​τ​s​​​​A​s​​​​ dAtdt=−Atτt \\frac {dA_t} {dt} = - \\frac {A_t} {\\tau_t} ​dt​​dA​t​​​​=−​τ​t​​​​A​t​​​​ if (pre fire), then{s←s+wAs←As+ΔAsw←w−At \\text{if (pre fire), then} \\begin{cases} s \\leftarrow s + w \\\\ A_s \\leftarrow A_s + \\Delta A_s \\\\ w \\leftarrow w - A_t \\end{cases} if (pre fire), then​⎩​⎪​⎨​⎪​⎧​​​s←s+w​A​s​​←A​s​​+ΔA​s​​​w←w−A​t​​​​ if (post fire), then{At←At+ΔAtw←w+As \\text{if (post fire), then} \\begin{cases} A_t \\leftarrow A_t + \\Delta A_t \\\\ w \\leftarrow w + A_s \\end{cases} if (post fire), then{​A​t​​←A​t​​+ΔA​t​​​w←w+A​s​​​​ 其中www为突触权重，sss与上一节讨论的一样为门控变量。与STP模型类似，这里由AsA_{s}A​s​​和AtA_{t}A​t​​两个变量分别控制LTD和LTP。ΔAs\\Delta A_sΔA​s​​ 和 ΔAt\\Delta A_tΔA​t​​分别为AsA_{s}A​s​​ 和 AtA_{t}A​t​​的增量，而τs\\tau_sτ​s​​ 和 τt\\tau_tτ​t​​则分别为它们的时间常数。 根据这个模型，当突触前神经元先于突触后神经元发放时，在突触后神经元发放之前，每当突触前神经元有一个脉冲，AsA_sA​s​​便增加，而由于此时突触后神经元没有脉冲，因此AtA_tA​t​​保持在初始值0，www暂时不会有变化。直到突触后神经元发放时，www的增量将会是As−AtA_s - A_tA​s​​−A​t​​，由于As>AtA_s>A_tA​s​​>A​t​​，此时会表现出长时程增强（LTP）。反之亦然。 现在让我们看看如何使用BrainPy来实现这个模型。其中sss动力学的实现部分，我们跟STP模型一样采用单指数衰减模型。 我们通过给予突触前和突触后的两群神经元不同的电流输入来控制它们产生脉冲的时间。首先我们在t=5mst=5mst=5ms时刻给突触前神经元第一段电流（每一段强度为30 μA\\mu AμA，并持续15ms，保证LIF模型会产生一个脉冲），然后在t=10mst=10mst=10ms才给突触后神经元一个输入。每段输入之间间隔15ms15ms15ms。以此在前三对脉冲中保持tpost=tpre+5t_{post}=t_{pre}+5t​post​​=t​pre​​+5。接下来我们设置一个较长的间隔，然后把刺激顺序调整为tpost=tpre−3t_{post}=t_{pre}-3t​post​​=t​pre​​−3。 duration = 300. (I_pre, _) = bp.inputs.constant_current([(0, 5), (30, 15), # pre at 5ms (0, 15), (30, 15), (0, 15), (30, 15), (0, 98), (30, 15), # switch order: t_interval=98ms (0, 15), (30, 15), (0, 15), (30, 15), (0, duration-155-98)]) (I_post, _) = bp.inputs.constant_current([(0, 10), (30, 15), # post at 10 (0, 15), (30, 15), (0, 15), (30, 15), (0, 90), (30, 15), # switch order: t_interval=98-8=90(ms) (0, 15), (30, 15), (0, 15), (30, 15), (0, duration-160-90)]) 接下来跑模拟的代码和STP类似，这里我们画出突触前后神经元的脉冲时间以及sss和www随时间的变化。 pre = bm.neurons.LIF(1, monitors=['spike']) post = bm.neurons.LIF(1, monitors=['spike']) syn = STDP(pre=pre, post=post, conn=bp.connect.All2All(), monitors=['s', 'w']) net = bp.Network(pre, syn, post) net.run(duration, inputs=[(pre, 'input', I_pre), (post, 'input', I_post)]) # plot fig, gs = bp.visualize.get_figure(4, 1, 2, 7) def hide_spines(my_ax): plt.legend() plt.xticks([]) plt.yticks([]) my_ax.spines['left'].set_visible(False) my_ax.spines['right'].set_visible(False) my_ax.spines['bottom'].set_visible(False) my_ax.spines['top'].set_visible(False) ax=fig.add_subplot(gs[0, 0]) plt.plot(net.ts, syn.mon.s[:, 0], label=\"s\") hide_spines(ax) ax1=fig.add_subplot(gs[1, 0]) plt.plot(net.ts, pre.mon.spike[:, 0], label=\"pre spike\") plt.ylim(0, 2) hide_spines(ax1) plt.legend(loc = 'center right') ax2=fig.add_subplot(gs[2, 0]) plt.plot(net.ts, post.mon.spike[:, 0], label=\"post spike\") plt.ylim(-1, 1) hide_spines(ax2) ax3=fig.add_subplot(gs[3, 0]) plt.plot(net.ts, syn.mon.w[:, 0], label=\"w\") plt.legend() # hide spines plt.yticks([]) ax3.spines['left'].set_visible(False) ax3.spines['right'].set_visible(False) ax3.spines['top'].set_visible(False) plt.xlabel('Time (ms)') plt.show() 结果正如我们所预期的，在150ms前，突触前神经元的脉冲时间在突触后神经元之前，www增加，呈现LTP。而150ms后，突触后神经元先于突触前神经元发放，www减少，呈现LTD。 Oja法则 接下来我们看基于赫布学习律（Hebbian learning）的发放率模型 (firing rate model)。赫布学习律认为相互连接的两个神经元在经历同步的放电活动后，它们之间的突触连接就会得到增强。而这个同步不需要在意两个神经元前后发放的次序，因此可以忽略具体的发放时间，简化为发放率模型。我们首先看赫布学习律的一般形式，对于jjj到iii的连接，用rjr_jr​j​​和rir_ir​i​​分别表示前神经元组和后神经元组的发放率，根据赫布学习律的局部性（locality）特性，wijw_{ij}w​ij​​的变化受www本身及rj,rir_j, r_ir​j​​,r​i​​的影响，得以下微分方程： ddtwij=F(wij;ri,rj) \\frac d {dt} w_{ij} = F(w_{ij}; r_{i},r_j) ​dt​​d​​w​ij​​=F(w​ij​​;r​i​​,r​j​​) 把上式右边经过泰勒展开可得下式： ddtwij=c00wij+c10wijrj+c01wijri+c20wijrj2+c02wijri2+c11wijrirj+O(r3) \\frac d {dt} w_{ij} = c_{00} w_{ij} + c_{10} w_{ij} r_j + c_{01} w_{ij} r_i + c_{20} w_{ij} r_j ^2 + c_{02} w_{ij} r_i ^2 + c_{11} w_{ij} r_i r_j + O(r^3) ​dt​​d​​w​ij​​=c​00​​w​ij​​+c​10​​w​ij​​r​j​​+c​01​​w​ij​​r​i​​+c​20​​w​ij​​r​j​2​​+c​02​​w​ij​​r​i​2​​+c​11​​w​ij​​r​i​​r​j​​+O(r​3​​) 赫布学习律的关键在于第六项，只有当第六项的系数c11c_{11}c​11​​非0才满足赫布学习律的同步发放。上式给出了赫布学习律的一般形式，接下来我们看一个具体的例子。 Oja法则的公式如下，对应于上式第5、6项系数非零，其中γ\\gammaγ为学习速率（learning rate）。 ddtwij=γ[rirj−wijri2] \\frac d {dt} w_{ij} = \\gamma [r_i r_j - w_{ij} r_i ^2 ] ​dt​​d​​w​ij​​=γ[r​i​​r​j​​−w​ij​​r​i​2​​] 下面我们用BrainPy来实现Oja法则。 由于Oja法则是发放率模型，它需要突触前后神经元具有变量rrr，因此我们定义一个简单的发放率神经元模型来观察两组神经元的学习规则。 我们打算实现如图2-3所示的连接。突触后神经元群iii（紫色）同时接受两群神经元j1j_1j​1​​（蓝色）和j2j_2j​2​​（红色）的输入。我们给iii和j2j_2j​2​​完全相同的刺激，而给j1j_1j​1​​的刺激一开始跟iii一致，但后来就不一致了。因此，根据赫布学习律，我们预期同步发放时，突触权重www会增加，当j1j_1j​1​​不再与iii同步发放时，则wij1w_{ij_1}w​ij​1​​​​停止增加。 图2-3 神经元的连接 从结果可以看到，在前100ms内，j1j_1j​1​​和j2j_2j​2​​均与iii同步发放，他们对应的w1w_1w​1​​和w2w_2w​2​​也同步增加，显示出LTP。而100ms后，j1j_1j​1​​（蓝色）不再发放，只有j2j_2j​2​​（红色）与iii同步发放，因此w1w_1w​1​​不再增加，w2w_2w​2​​则持续增加。该结果符合赫布学习律。 BCM法则 现在我们来看赫布学习律的另一个例子——BCM法则。它的公式如下： ddtwij=ηri(ri−rθ)rj \\frac d{dt} w_{ij} = \\eta r_i(r_i - r_\\theta) r_j ​dt​​d​​w​ij​​=ηr​i​​(r​i​​−r​θ​​)r​j​​ 其中η\\etaη为学习速率，rθr_\\thetar​θ​​为学习的阈值（见图2-4）。图2-4画出了上式的右边，当发放频率高于阈值时呈现LTP，低于阈值时则为LTD。因此，这是一种频率依赖可塑性（回想一下，前面介绍的STDP为时间依赖可塑性），可以通过调整阈值rθr_\\thetar​θ​​实现选择性。 图2-4 BCM法则 (改编自 Gerstner et al., 2014 1) 我们将实现和Oja法则相同的连接方式（图2-3），但给的刺激不同。在这里，我们让j1j_1j​1​​（蓝色）和j2j_2j​2​​（红色）交替发放，且j1j_1j​1​​的发放率比j2j_2j​2​​高。我们动态调整阈值为rir_ir​i​​的时间平均，即 rθ=f(ri)=∫dtriTr_\\theta = f(r_i)=\\frac {\\int dt r_i}Tr​θ​​=f(r​i​​)=​T​​∫dtr​i​​​​。BrainPy实现的代码如下。 定义了BCM类以后，我们可以跑模拟了。 n_post = 1 n_pre = 20 # group selection group1, duration = bp.inputs.constant_current(([1.5, 1], [0, 1]) * 20) group2, duration = bp.inputs.constant_current(([0, 1], [1., 1]) * 20) group1 = bp.ops.vstack(((group1,)*10)) group2 = bp.ops.vstack(((group2,)*10)) input_r = bp.ops.vstack((group1, group2)) pre = neu(n_pre, monitors=['r']) post = neu(n_post, monitors=['r']) bcm = BCM(pre=pre, post=post,conn=bp.connect.All2All(), monitors=['w']) net = bp.Network(pre, bcm, post) net.run(duration, inputs=(pre, 'r', input_r.T, \"=\")) w1 = bp.ops.mean(bcm.mon.w[:, :10, 0], 1) w2 = bp.ops.mean(bcm.mon.w[:, 10:, 0], 1) r1 = bp.ops.mean(pre.mon.r[:, :10], 1) r2 = bp.ops.mean(pre.mon.r[:, 10:], 1) fig, gs = bp.visualize.get_figure(2, 1, 3, 12) fig.add_subplot(gs[1, 0], xlim=(0, duration), ylim=(0, w_max)) plt.plot(net.ts, w1, 'b', label='w1') plt.plot(net.ts, w2, 'r', label='w2') plt.title(\"weights\") plt.ylabel(\"weights\") plt.xlabel(\"t\") plt.legend() fig.add_subplot(gs[0, 0], xlim=(0, duration)) plt.plot(net.ts, r1, 'b', label='r1') plt.plot(net.ts, r2, 'r', label='r2') plt.title(\"inputs\") plt.ylabel(\"firing rate\") plt.xlabel(\"t\") plt.legend() plt.show() 结果显示，每次发放率都比较高的j1j_1j​1​​（蓝色），其对应的w1w_1w​1​​持续增加，显示出LTP。而w2w_2w​2​​则呈现出LTD，这个结果显示出BCM法则的选择功能。 参考资料 1. Gerstner, Wulfram, et al. Neuronal dynamics: From single neurons to networks and models of cognition. Cambridge University Press, 2014. ↩ 2. Bi, Guo-qiang, and Mu-ming Poo. \"Synaptic modification by correlated activity: Hebb's postulate revisited.\" Annual review of neuroscience 24.1 (2001): 139-166. ↩ "},"networks.html":{"url":"networks.html","title":"3. 网络模型","keywords":"","body":"3. 网络模型 到此，读者已经了解了几种最常见、最经典的神经元和突触模型，是时候更进一步了。本节中，我们将会介绍计算神经科学中两种重要的网络模型：脉冲神经网络和发放率神经网络。 脉冲神经网络的特点是，网络分别建模、计算每个神经元和突触；而发放率网络则把将一群神经元简化为单个发放率单元，并以一个发放率单元中的神经元群作为计算的最小单位。 3.1 脉冲神经网络 3.2 发放率神经网络 "},"networks/spiking_neural_networks.html":{"url":"networks/spiking_neural_networks.html","title":"3.1 脉冲神经网络","keywords":"","body":"3.1 脉冲神经网络 3.1.1 兴奋-抑制平衡网络 上世纪90年代，生物学实验上发现，在大脑皮层中的神经活动经常表现出一种时间上无规则的发放模式。这种发放模式广泛地存在在脑区中，但当时学界对它的机制和功能都了解不多。 Vreeswijk和Sompolinsky（1996）提出了兴奋-抑制平衡网络（E/I balanced network）来解释这种不规则的发放模式。兴奋-抑制平衡网络的结构特征是神经元之间强的随机稀疏突触连接，由于这种结构和对应的参数设置，网络中每个神经元都会接收到很大的来自网络内部的兴奋性和抑制性输入。但是，这两种输入将会相互抵消，最后神经元接收到的总输入将保持在一个相对小的数量级上，仅足以让膜电位上升到阈值电位，引发神经元的发放。 网络结构上的随机性和噪声的共同作用，使得兴奋-抑制平衡网络中的每一个神经元都接收到随时间和空间变化的、阈值电位量级的输入。这样，神经元的发放也具有了随机性，保证兴奋-抑制平衡网络能够自发产生时间上不规则的发放模式。 Fig.3-1 Structure of E/I balanced network | Vreeswijk and Sompolinsky, 1996 Vreeswijk和Sompolinsky同时也提出了这种发放模式一个可能的功能：兴奋-抑制平衡网络可以对外部刺激的变化快速做出反应。 如图3-3所示，当没有外部输入时，兴奋-抑制平衡网络中的神经元膜电位在静息电位V0V_0V​0​​和阈值电位θ\\thetaθ之间服从一个相对均匀的随机分布。 Fig.3-2 Distribution of neuron membrane potentials in E/I balanced network | Tian et al., 2020 当我们给网络一个小的恒定外部输入时，那些膜电位原本就落在阈值电位附近的神经元收到这一输入就会很快发放。在网络尺度上，表现为网络的发放率随输入变化而快速改变。 仿真证实，在这种情况下，网络对输入反应的延迟时长，和突触延迟的时长在同一量级，并且二者都远小于一个单神经元面对同样大小的外部输入、从静息电位积累到产生动作电位所需的时间。因此，兴奋-抑制平衡网络可能为神经网络提供了一种快速反应的机制。 图3-1画出了兴奋-抑制平衡网络的结构： 1）神经元实现为LIF模型。神经元可以被分为兴奋性神经元和抑制性神经元，两种神经元的比例是NEN_EN​E​​: NIN_IN​I​​ = 4:1。 2）突触实现为指数型突触。在上述的两类神经元中，产生了四类突触连接：兴奋-兴奋连接（E2E conn），兴奋-抑制连接（E2I conn），抑制-兴奋连接（I2E conn），抑制-抑制连接（I2I conn）。定义符号相反的突触权重来区分兴奋性和抑制型的突触连接。 3）输入：网络中的所有神经元接受到一个恒定的外部输入。 LIF神经元和指数型突触的实现请参见上面两节。在仿真完成后，我们画出兴奋-抑制平衡网络的发放情况，可以看到网络从一开始的强同步性发放渐渐变为无规则波动。 Fig.3-3 E/I balanced net raster plot 3.1.2 决策网络 计算神经科学的网络建模也可以对标特定的生理实验任务。比如，在视觉运动区分任务（Roitman和Shadlen，2002）中，猕猴将观看一段视频，视频中特定区域内的随机点以一定比例向左或向右运动。猕猴被要求判断朝哪个方向运动的点更多，并通过眼动给出答案。同时，研究者用植入电极记录猕猴LIP神经元的活动。 Fig.3-4 Experimental Diagram Wang（2002）提出了决策网络来建模在如上任务的决策过程中，猕猴LIP神经元的活动。 如图3-5所示，网络同样基于兴奋-抑制平衡网络。兴奋性神经元和抑制型神经元的数量比是NE:NI=4:1N_E:N_I = 4:1N​E​​:N​I​​=4:1，调整参数使得网络处在平衡状态下。 为了完成决策任务，在兴奋性神经元群中，选出两个选择性子神经元群A和B，大小均为兴奋性神经元群的0.15倍（NA=NB=0.15NEN_A = N_B = 0.15N_EN​A​​=N​B​​=0.15N​E​​）。这两个子神经元群在下图中被标为A和B，其他的兴奋性神经元被称为非选择性的神经元，其数目为Nnon=(1−2∗0.15)NEN_{non} = (1-2*0.15)N_EN​non​​=(1−2∗0.15)N​E​​。 Fig.3-5 structure of decision makingnetwork 决策网络中共有四组突触——E2E，E2I，I2E和I2I突触连接，其中兴奋性突触实现为AMPA突触，抑制性突触实现为GABAa突触。 由于网络需要在两个选项（子神经元群A和B）之间做出决策，必须要区分这两个子神经元群。一个选择性的子神经元群应当激活自身，并同时抑制另一个子神经元群。 因此，网络中的E2E连接被建模为有结构的连接。如表3-1所示，w+>1>w−w+ > 1 > w-w+>1>w−。通过这种方法，在一个选择性子神经元群之内，通过更强的兴奋性突触连接达成了一种相对的激活，而在选择性子神经元群之间或是选择性和非选择性子神经元群间，更弱的兴奋性突触连接实际上形成了相对的抑制。A和B两个神经元因此形成了竞争关系，迫使网络做出二选一的决策。 Sheet 3-1 Weight of synapse connections between E-neurons 决策网络收到两种外部输入： 1）从其他脑区传来的非特定的背景输入，表示为AMPA突触介导的高频泊松输入（2400Hz）。 2）仅两个选择性子神经元群A和B收到的外部传来的刺激输入。表示为AMPA突触介导的较低频率的泊松输入。 给予A和B神经元群的泊松输入的频率（μA\\mu_Aμ​A​​、μB\\mu_Bμ​B​​）有一定差别，对应着生理实验中朝两个方向运动的随机点的比例差别，引导网络在两个子神经元群中进行决策。 ρA=ρB=μ0/100 \\rho_A = \\rho_B = \\mu_0/100 ρ​A​​=ρ​B​​=μ​0​​/100 μA=μ0+ρA∗c \\mu_A = \\mu_0 + \\rho_A * c μ​A​​=μ​0​​+ρ​A​​∗c μB=μ0+ρB∗c \\mu_B = \\mu_0 + \\rho_B * c μ​B​​=μ​0​​+ρ​B​​∗c 每50毫秒，泊松输入的频率fxf_xf​x​​遵循由均值μx\\mu_xμ​x​​ 和方差δ2\\delta^2δ​2​​定义的高斯分布，重新进行一次随机采样。 fA∼N(μA,δ2) f_A \\sim N(\\mu_A, \\delta^2) f​A​​∼N(μ​A​​,δ​2​​) fB∼N(μB,δ2) f_B \\sim N(\\mu_B, \\delta^2) f​B​​∼N(μ​B​​,δ​2​​) 在仿真时，子神经元群A收到的刺激输入比B收到的更大。在一定延迟时间之后，A群的活动水平明显高于B群，说明网络做出了正确的选择。 Fig.3-6 decision making network "},"networks/rate_models.html":{"url":"networks/rate_models.html","title":"3.2 发放率神经网络","keywords":"","body":"发放率网络 Firing rate networks 决择模型 除了脉冲模型以外，BrainPy同样也可以实现发放率模型（Firing rate models）。我们首先来看看前述抉择模型的简化版本的实现。该模型由研究者（Wong & Wang, 2006）通过平均场方法（mean-field approach）等一系列手段简化得出，最终只剩下两个变量，S1S_1S​1​​和S2S_2S​2​​，分别表示两群神经元的状态，同时也分别对应着两个选项。 公式如下： dS1dt=−S1τ+(1−S1)γr1 \\frac{dS_1} {dt} = -\\frac {S_1} \\tau + (1-S_1) \\gamma r_1 ​dt​​dS​1​​​​=−​τ​​S​1​​​​+(1−S​1​​)γr​1​​ dS2dt=−S2τ+(1−S2)γr2 \\frac{dS_2} {dt} = -\\frac {S_2} \\tau + (1-S_2) \\gamma r_2 ​dt​​dS​2​​​​=−​τ​​S​2​​​​+(1−S​2​​)γr​2​​ 其中 r1r_1r​1​​ 和 r2r_2r​2​​ 分别为两群神经元的发放率，由输入-输出函数（input-output function）给出，为： ri=f(Isyn,i) r_i = f(I_{syn, i}) r​i​​=f(I​syn,i​​) f(I)=aI−b1−exp[−d(aI−b)] f(I)= \\frac {aI-b} {1- \\exp [-d(aI-b)]} f(I)=​1−exp[−d(aI−b)]​​aI−b​​ Isyn,iI_{syn, i}I​syn,i​​ 的公式由模型结构给出，为自身的循环（recurrent）连接减去对方传来的抑制电流，并加上背景电流及外界输入，可得： Isyn,1=J11S1−J12S2+I0+I1 I_{syn, 1} = J_{11} S_1 - J_{12} S_2 + I_0 + I_1 I​syn,1​​=J​11​​S​1​​−J​12​​S​2​​+I​0​​+I​1​​ Isyn,2=J22S2−J21S1+I0+I2 I_{syn, 2} = J_{22} S_2 - J_{21} S_1 + I_0 + I_2 I​syn,2​​=J​22​​S​2​​−J​21​​S​1​​+I​0​​+I​2​​ 而外界输入 I1,I2I_1, I_2I​1​​,I​2​​ 则由总输入的强度 μ0\\mu_0μ​0​​ 及一致性（coherence） c′c'c​′​​ 决定。一致性越高，则越明确S1S_1S​1​​是正确答案，而一致性越低则表示越随机。公式如下： I1=JA, extμ0(1+c′100%) I_1 = J_{\\text{A, ext}} \\mu_0 (1+\\frac {c'}{100\\%}) I​1​​=J​A, ext​​μ​0​​(1+​100%​​c​′​​​​) I2=JA, extμ0(1−c′100%) I_2 = J_{\\text{A, ext}} \\mu_0 (1-\\frac {c'}{100\\%}) I​2​​=J​A, ext​​μ​0​​(1−​100%​​c​′​​​​) 代码实现如下，我们可以创建一个神经元群的类（bp.NeuGroup），并用S1S_1S​1​​及S2S_2S​2​​分别储存这群神经元的两个状态。该模型的动力学部分可以由一个derivative函数实现，以便进行动力学分析。 class Decision(bp.NeuGroup): target_backend = 'general' @staticmethod def derivative(s1, s2, t, I, coh, JAext, J_rec, J_inh, I_0, b, d, a, tau_s, gamma): I1 = JAext * I * (1. + coh) I2 = JAext * I * (1. - coh) I_syn1 = J_rec * s1 - J_inh * s2 + I_0 + I1 r1 = (a * I_syn1 - b) / (1. - bp.ops.exp(-d * (a * I_syn1 - b))) ds1dt = - s1 / tau_s + (1. - s1) * gamma * r1 I_syn2 = J_rec * s2 - J_inh * s1 + I_0 + I2 r2 = (a * I_syn2 - b) / (1. - bp.ops.exp(-d * (a * I_syn2 - b))) ds2dt = - s2 / tau_s + (1. - s2) * gamma * r2 return ds1dt, ds2dt def __init__(self, size, coh, tau_s=.06, gamma=0.641, J_rec=.3725, J_inh=.1137, I_0=.3297, JAext=.00117, a=270., b=108., d=0.154, **kwargs): # parameters self.coh = coh self.tau_s = tau_s self.gamma = gamma self.J_rec = J_rec self.J_inh = J_inh self.I0 = I_0 self.JAext = JAext self.a = a self.b = b self.d = d # variables self.s1 = bp.ops.ones(size) * .06 self.s2 = bp.ops.ones(size) * .06 self.input = bp.ops.zeros(size) self.integral = bp.odeint(f=self.derivative, method='rk4', dt=0.01) super(Decision, self).__init__(size=size, **kwargs) def update(self, _t): for i in prange(self.size): self.s1[i], self.s2[i] = self.integral(self.s1[i], self.s2[i], _t, self.input[i], self.coh, self.JAext, self.J_rec, self.J_inh, self.I0, self.b, self.d, self.a, self.tau_s, self.gamma) self.input[i] = 0. 相平面分析的代码如下。让我们来看看当没有外界输入，即μ0=0\\mu_0 = 0μ​0​​=0时的动力学。 from collections import OrderedDict pars = dict(tau_s=.06, gamma=0.641, J_rec=.3725, J_inh=.1137, I_0=.3297, JAext=.00117, b=108., d=0.154, a=270.) pars['I'] = 0. pars['coh'] = 0. decision = Decision(1, coh=pars['coh']) phase = bp.analysis.PhasePlane(decision.integral, target_vars=OrderedDict(s2=[0., 1.], s1=[0., 1.]), fixed_vars=None, pars_update=pars, numerical_resolution=.001, options={'escape_sympy_solver': True}) phase.plot_nullcline() phase.plot_fixed_point() phase.plot_vector_field(show=True) plot nullcline ... plot fixed point ... Fixed point #1 at s2=0.06176109215560733, s1=0.061761097890810475 is a stable node. Fixed point #2 at s2=0.029354239100062428, s1=0.18815448592736211 is a saddle node. Fixed point #3 at s2=0.0042468423702408655, s1=0.6303045696241589 is a stable node. Fixed point #4 at s2=0.6303045696241589, s1=0.004246842370235128 is a stable node. Fixed point #5 at s2=0.18815439944520335, s1=0.029354240536530615 is a saddle node. plot vector field ... 由此可见，用BrainPy进行动力学分析是非常方便的。向量场和不动点 (fixed point)表示了不同初始值下最终会落在哪个选项。 这里，x轴是S2S_2S​2​​，代表选项2，y轴是S1S_1S​1​​，代表选项1。可以看到，左上的不动点 表示选项1，右下的不动点表示选项2，左下的不动点表示没有选择。 现在让我们看看当我们把外部输入强度固定为30时，在不同一致性（coherence）下的相平面。 # coherence = 0% pars['I'] = 30. pars['coh'] = 0. decision = Decision(1, coh=pars['coh']) phase = bp.analysis.PhasePlane(decision.integral, target_vars=OrderedDict(s2=[0., 1.], s1=[0., 1.]), fixed_vars=None, pars_update=pars, numerical_resolution=.001, options={'escape_sympy_solver': True}) print(\"coherence = 0%\") phase.plot_nullcline() phase.plot_fixed_point() phase.plot_vector_field(show=True) # coherence = 51.2% pars['coh'] = 0.512 decision = Decision(1, coh=pars['coh']) phase = bp.analysis.PhasePlane(decision.integral, target_vars=OrderedDict(s2=[0., 1.], s1=[0., 1.]), fixed_vars=None, pars_update=pars, numerical_resolution=.001, options={'escape_sympy_solver': True}) print(\"coherence = 51.2%\") phase.plot_nullcline() phase.plot_fixed_point() phase.plot_vector_field(show=True) # coherence = 100% pars['coh'] = 1. decision = Decision(1, coh=pars['coh']) phase = bp.analysis.PhasePlane(decision.integral, target_vars=OrderedDict(s2=[0., 1.], s1=[0., 1.]), fixed_vars=None, pars_update=pars, numerical_resolution=.001, options={'escape_sympy_solver': True}) print(\"coherence = 100%\") phase.plot_nullcline() phase.plot_fixed_point() phase.plot_vector_field(show=True) coherence = 0% plot nullcline ... plot fixed point ... Fixed point #1 at s2=0.6993504413889349, s1=0.011622049526766405 is a stable node. Fixed point #2 at s2=0.49867489858358865, s1=0.49867489858358865 is a saddle node. Fixed point #3 at s2=0.011622051540013889, s1=0.6993504355529329 is a stable node. plot vector field ... coherence = 51.2% plot nullcline ... plot fixed point ... Fixed point #1 at s2=0.5673124813731691, s1=0.2864701069327971 is a saddle node. Fixed point #2 at s2=0.6655747347157656, s1=0.027835279565912054 is a stable node. Fixed point #3 at s2=0.005397687847426814, s1=0.7231453520305031 is a stable node. plot vector field ... coherence = 100% plot nullcline ... plot fixed point ... Fixed point #1 at s2=0.0026865954387078755, s1=0.7410985604497689 is a stable node. plot vector field ... 连续吸引子模型（CANN） 让我们看看发放率模型（firing rate model）的另一个例子——连续吸引子神经网络（CANN）。一维CANN的结构如下： 神经元群的突触总输入uuu的动力学方程如下： τdu(x,t)dt=−u(x,t)+ρ∫dx′J(x,x′)r(x′,t)+Iext \\tau \\frac{du(x,t)}{dt} = -u(x,t) + \\rho \\int dx' J(x,x') r(x',t)+I_{ext} τ​dt​​du(x,t)​​=−u(x,t)+ρ∫dx​′​​J(x,x​′​​)r(x​′​​,t)+I​ext​​ 其中x表示神经元群的参数空间位点，r(x′,t)r(x', t)r(x​′​​,t)是神经元群(x')的发放率，由以下公式给出: r(x,t)=u(x,t)21+kρ∫dx′u(x′,t)2 r(x,t) = \\frac{u(x,t)^2}{1 + k \\rho \\int dx' u(x',t)^2} r(x,t)=​1+kρ∫dx​′​​u(x​′​​,t)​2​​​​u(x,t)​2​​​​ 而神经元群(x)和(x')之间的兴奋性连接强度J(x,x′)J(x, x')J(x,x​′​​)由高斯函数给出: J(x,x′)=12πaexp(−∣x−x′∣22a2) J(x,x') = \\frac{1}{\\sqrt{2\\pi}a}\\exp(-\\frac{|x-x'|^2}{2a^2}) J(x,x​′​​)=​√​2π​​​a​​1​​exp(−​2a​2​​​​∣x−x​′​​∣​2​​​​) 外界输入IextI_{ext}I​ext​​与位置z(t)z(t)z(t)有关，公式如下： Iext=Aexp[−∣x−z(t)∣24a2] I_{ext} = A\\exp\\left[-\\frac{|x-z(t)|^2}{4a^2}\\right] I​ext​​=Aexp[−​4a​2​​​​∣x−z(t)∣​2​​​​] 在BrainPy的实现上，我们通过继承bp.NeuGroup来创建一个CANN1D的类： class CANN1D(bp.NeuGroup): target_backend = ['numpy', 'numba'] @staticmethod def derivative(u, t, conn, k, tau, Iext): r1 = np.square(u) r2 = 1.0 + k * np.sum(r1) r = r1 / r2 Irec = np.dot(conn, r) du = (-u + Irec + Iext) / tau return du def __init__(self, num, tau=1., k=8.1, a=0.5, A=10., J0=4., z_min=-np.pi, z_max=np.pi, **kwargs): # parameters self.tau = tau # The synaptic time constant self.k = k # Degree of the rescaled inhibition self.a = a # Half-width of the range of excitatory connections self.A = A # Magnitude of the external input self.J0 = J0 # maximum connection value # feature space self.z_min = z_min self.z_max = z_max self.z_range = z_max - z_min self.x = np.linspace(z_min, z_max, num) # The encoded feature values # variables self.u = np.zeros(num) self.input = np.zeros(num) # The connection matrix self.conn_mat = self.make_conn(self.x) self.int_u = bp.odeint(f=self.derivative, method='rk4', dt=0.05) super(CANN1D, self).__init__(size=num, **kwargs) self.rho = num / self.z_range # The neural density self.dx = self.z_range / num # The stimulus density def dist(self, d): d = np.remainder(d, self.z_range) d = np.where(d > 0.5 * self.z_range, d - self.z_range, d) return d def make_conn(self, x): assert np.ndim(x) == 1 x_left = np.reshape(x, (-1, 1)) x_right = np.repeat(x.reshape((1, -1)), len(x), axis=0) d = self.dist(x_left - x_right) Jxx = self.J0 * np.exp(-0.5 * np.square(d / self.a)) / (np.sqrt(2 * np.pi) * self.a) return Jxx def get_stimulus_by_pos(self, pos): return self.A * np.exp(-0.25 * np.square(self.dist(self.x - pos) / self.a)) def update(self, _t): self.u = self.int_u(self.u, _t, self.conn_mat, self.k, self.tau, self.input) self.input[:] = 0. 其中函数dist与make_conn用来计算两群神经元之间的连接强度JJJ。在make_conn函数中，我们首先计算每两个xxx之间的距离矩阵。由于神经元群是环状排列的，xxx的值介于−π-\\pi−π到π\\piπ之间，所以x−x′x-x'x−x​′​​的范围为2π2\\pi2π，且−π-\\pi−π和π\\piπ是同一个点（实际最远是π\\piπ，即0.5*z_range，超出的距离需要减去一个z_range）。我们用dist函数来处理环上的距离。 而get_stimulus_by_pos函数则是根据参数空间位点pos处理外界输入，可供用户在使用时调用获取所需的输入电流大小。例如在简单的群编码（population coding）中，我们给一个pos=0的外界输入，并按以下方式运行： cann = CANN1D(num=512, k=0.1, monitors=['u']) I1 = cann.get_stimulus_by_pos(0.) Iext, duration = bp.inputs.constant_current([(0., 1.), (I1, 8.), (0., 8.)]) cann.run(duration=duration, inputs=('input', Iext)) bp.visualize.animate_1D( dynamical_vars=[{'ys': cann.mon.u, 'xs': cann.x, 'legend': 'u'}, {'ys': Iext, 'xs': cann.x, 'legend': 'Iext'}], frame_step=1, frame_delay=100, show=True, save_path='figs/CANN-encoding.gif' ) 可以看到，uuu的形状编码了外界输入的形状。 现在我们给外界输入加上随机噪声，看看uuu的形状如何变化。 cann = CANN1D(num=512, k=8.1, monitors=['u']) dur1, dur2, dur3 = 10., 30., 0. num1 = int(dur1 / bp.backend.get_dt()) num2 = int(dur2 / bp.backend.get_dt()) num3 = int(dur3 / bp.backend.get_dt()) Iext = np.zeros((num1 + num2 + num3,) + cann.size) Iext[:num1] = cann.get_stimulus_by_pos(0.5) Iext[num1:num1 + num2] = cann.get_stimulus_by_pos(0.) Iext[num1:num1 + num2] += 0.1 * cann.A * np.random.randn(num2, *cann.size) cann.run(duration=dur1 + dur2 + dur3, inputs=('input', Iext)) bp.visualize.animate_1D( dynamical_vars=[{'ys': cann.mon.u, 'xs': cann.x, 'legend': 'u'}, {'ys': Iext, 'xs': cann.x, 'legend': 'Iext'}], frame_step=5, frame_delay=50, show=True, save_path='figs/CANN-decoding.gif' ) 我们可以看到uuu的形状保持一个类似高斯的钟形，这表明CANN可以进行模版匹配。 接下来我们用np.linspace函数来产生不同的位置，得到随时间平移的输入，我们将会看到uuu跟随着外界输入移动，即平滑追踪。 cann = CANN1D(num=512, k=8.1, monitors=['u']) dur1, dur2, dur3 = 20., 20., 20. num1 = int(dur1 / bp.backend.get_dt()) num2 = int(dur2 / bp.backend.get_dt()) num3 = int(dur3 / bp.backend.get_dt()) position = np.zeros(num1 + num2 + num3) position[num1: num1 + num2] = np.linspace(0., 12., num2) position[num1 + num2:] = 12. position = position.reshape((-1, 1)) Iext = cann.get_stimulus_by_pos(position) cann.run(duration=dur1 + dur2 + dur3, inputs=('input', Iext)) bp.visualize.animate_1D( dynamical_vars=[{'ys': cann.mon.u, 'xs': cann.x, 'legend': 'u'}, {'ys': Iext, 'xs': cann.x, 'legend': 'Iext'}], frame_step=5, frame_delay=50, show=True, save_path='figs/CANN-tracking.gif' ) "}}